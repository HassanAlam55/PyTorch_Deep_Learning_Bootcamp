{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/07_pytorch_experiment_tracking_exercise_template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNqPNlYylluR"
      },
      "source": [
        "# 07. PyTorch Experiment Tracking Exercise Template\n",
        "\n",
        "Welcome to the 07. PyTorch Experiment Tracking exercise template notebook.\n",
        "\n",
        "> **Note:** There may be more than one solution to each of the exercises. This notebook only shows one possible example.\n",
        "\n",
        "## Resources\n",
        "\n",
        "1. These exercises/solutions are based on [section 07. PyTorch Transfer Learning](https://www.learnpytorch.io/07_pytorch_experiment_tracking/) of the Learn PyTorch for Deep Learning course by Zero to Mastery.\n",
        "2. See a live [walkthrough of the solutions (errors and all) on YouTube](https://youtu.be/cO_r2FYcAjU).\n",
        "3. See [other solutions on the course GitHub](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/extras/solutions).\n",
        "\n",
        "> **Note:** The first section of this notebook is dedicated to getting various helper functions and datasets used for the exercises. The exercises start at the heading \"Exercise 1: ...\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf8ab9cyHTzU"
      },
      "source": [
        "### Get various imports and helper functions\n",
        "\n",
        "We'll need to make sure we have `torch` v.1.12+ and `torchvision` v0.13+."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Tiger\\anaconda3\\envs\\p311Udacity1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from helper_functions import plot_predictions, plot_decision_boundary, accuracy_fn\n",
        "from going_modular import engine, data_loaders\n",
        "from going_modular.engine import train_step, test_step\n",
        "from going_modular.utils import save_model\n",
        "import mlxtend\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import random\n",
        "import requests\n",
        "import sklearn\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_blobs\n",
        "from torchinfo import summary\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchmetrics import Accuracy, ConfusionMatrix\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from timeit import default_timer as timer\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Tuple, Dict, List\n",
        "writer = SummaryWriter()\n",
        "import zipfile\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MOv1De4mxeL",
        "outputId": "a4f9fdd9-f225-4861-e204-e26186365bea"
      },
      "outputs": [],
      "source": [
        "# # For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
        "# try:\n",
        "#     import torch\n",
        "#     import torchvision\n",
        "#     assert int(torch.__version__.split(\".\")[1]) >= 12, \"torch version should be 1.12+\"\n",
        "#     assert int(torchvision.__version__.split(\".\")[1]) >= 13, \"torchvision version should be 0.13+\"\n",
        "#     print(f\"torch version: {torch.__version__}\")\n",
        "#     print(f\"torchvision version: {torchvision.__version__}\")\n",
        "# except:\n",
        "#     print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n",
        "#     !pip3 install -U --pre torch torchvision --extra-index-url https://download.pytorch.org/whl/nightly/cu113\n",
        "#     import torch\n",
        "#     import torchvision\n",
        "#     print(f\"torch version: {torch.__version__}\")\n",
        "#     print(f\"torchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch version: 2.3.0\n",
            "torchvision version: 0.18.0\n"
          ]
        }
      ],
      "source": [
        "print(f\"torch version: {torch.__version__}\")\n",
        "print(f\"torchvision version: {torchvision.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Nf-DsrZipCE9",
        "outputId": "f4cc7d1c-da78-4eb4-c753-4e135771650c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Make sure we have a GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "i_52puIeoab3"
      },
      "outputs": [],
      "source": [
        "# # Get regular imports \n",
        "# import matplotlib.pyplot as plt\n",
        "# import torch\n",
        "# import torchvision\n",
        "\n",
        "# from torch import nn\n",
        "# from torchvision import transforms\n",
        "\n",
        "# # Try to get torchinfo, install it if it doesn't work\n",
        "# try:\n",
        "#     from torchinfo import summary\n",
        "# except:\n",
        "#     print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "#     !pip install -q torchinfo\n",
        "#     from torchinfo import summary\n",
        "\n",
        "# # Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "# try:\n",
        "#     from going_modular.going_modular import data_setup, engine\n",
        "# except:\n",
        "#     # Get the going_modular scripts\n",
        "#     print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n",
        "#     !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "#     !mv pytorch-deep-learning/going_modular .\n",
        "#     !rm -rf pytorch-deep-learning\n",
        "#     from going_modular.going_modular import data_setup, engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DBj8I3P9pNK2"
      },
      "outputs": [],
      "source": [
        "# Set seeds\n",
        "def set_seeds(seed: int=42):\n",
        "    \"\"\"Sets random sets for torch operations.\n",
        "\n",
        "    Args:\n",
        "        seed (int, optional): Random seed to set. Defaults to 42.\n",
        "    \"\"\"\n",
        "    # Set the seed for general torch operations\n",
        "    torch.manual_seed(seed)\n",
        "    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n",
        "    torch.cuda.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6R-CS53pTLS",
        "outputId": "3f7688b7-0b86-4cd8-bb11-4e71f8e7270f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] ..\\data\\pizza_steak_sushi_1 directory exists, skipping download.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "WindowsPath('../data/pizza_steak_sushi_1')"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import requests\n",
        "\n",
        "def download_data(source: str, \n",
        "                  destination: str,\n",
        "                  data_dir = '../data',\n",
        "                  remove_source: bool = True) -> Path:\n",
        "    \"\"\"Downloads a zipped dataset from source and unzips to destination.\n",
        "\n",
        "    Args:\n",
        "        source (str): A link to a zipped file containing data.\n",
        "        destination (str): A target directory to unzip data to.\n",
        "        remove_source (bool): Whether to remove the source after downloading and extracting.\n",
        "    \n",
        "    Returns:\n",
        "        pathlib.Path to downloaded data.\n",
        "    \n",
        "    Example usage:\n",
        "        download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                      destination=\"pizza_steak_sushi\")\n",
        "    \"\"\"\n",
        "    # Setup path to data folder\n",
        "    data_path = Path(data_dir)\n",
        "    image_path = data_path / destination\n",
        "\n",
        "    # If the image folder doesn't exist, download it and prepare it... \n",
        "    if image_path.is_dir():\n",
        "        print(f\"[INFO] {image_path} directory exists, skipping download.\")\n",
        "    else:\n",
        "        print(f\"[INFO] Did not find {image_path} directory, creating one...\")\n",
        "        image_path.mkdir(parents=True, exist_ok=True)\n",
        "        \n",
        "        # Download pizza, steak, sushi data\n",
        "        target_file = Path(source).name\n",
        "        with open(data_path / target_file, \"wb\") as f:\n",
        "            request = requests.get(source)\n",
        "            print(f\"[INFO] Downloading {target_file} from {source}...\")\n",
        "            f.write(request.content)\n",
        "\n",
        "        # Unzip pizza, steak, sushi data\n",
        "        with zipfile.ZipFile(data_path / target_file, \"r\") as zip_ref:\n",
        "            print(f\"[INFO] Unzipping {target_file} data...\") \n",
        "            zip_ref.extractall(image_path)\n",
        "\n",
        "        # Remove .zip file\n",
        "        if remove_source:\n",
        "            os.remove(data_path / target_file)\n",
        "    \n",
        "    return image_path\n",
        "\n",
        "image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "                           destination=\"pizza_steak_sushi_1\")\n",
        "image_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BE60IEEkr89l"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "def create_writer(experiment_name: str, \n",
        "                  model_name: str, \n",
        "                  extra: str=None):\n",
        "    \"\"\"Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.\n",
        "\n",
        "    log_dir is a combination of runs/timestamp/experiment_name/model_name/extra.\n",
        "\n",
        "    Where timestamp is the current date in YYYY-MM-DD format.\n",
        "\n",
        "    Args:\n",
        "        experiment_name (str): Name of experiment.\n",
        "        model_name (str): Name of model.\n",
        "        extra (str, optional): Anything extra to add to the directory. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to log_dir.\n",
        "\n",
        "    Example usage:\n",
        "        # Create a writer saving to \"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\"\n",
        "        writer = create_writer(experiment_name=\"data_10_percent\",\n",
        "                               model_name=\"effnetb2\",\n",
        "                               extra=\"5_epochs\")\n",
        "        # The above is the same as:\n",
        "        writer = SummaryWriter(log_dir=\"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\")\n",
        "    \"\"\"\n",
        "    from datetime import datetime\n",
        "    import os\n",
        "\n",
        "    # Get timestamp of current date (all experiments on certain day live in same folder)\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d\") # returns current date in YYYY-MM-DD format\n",
        "\n",
        "    if extra:\n",
        "        # Create log directory path\n",
        "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)\n",
        "    else:\n",
        "        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n",
        "        \n",
        "    print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")\n",
        "    return SummaryWriter(log_dir=log_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0BH4ONGsgNB",
        "outputId": "077f7b39-f4d0-44dd-b74f-8ead04fb7add"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] Created SummaryWriter, saving to: runs\\2024-09-22\\test_experiment_name\\this_is_the_model_name\\add_a_little_extra_if_you_want...\n"
          ]
        }
      ],
      "source": [
        "# Create a test writer\n",
        "writer = create_writer(experiment_name=\"test_experiment_name\",\n",
        "                       model_name=\"this_is_the_model_name\",\n",
        "                       extra=\"add_a_little_extra_if_you_want\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VwO0Q1eFsusV"
      },
      "outputs": [],
      "source": [
        "# from typing import Dict, List\n",
        "# from tqdm.auto import tqdm\n",
        "\n",
        "# from going_modular.going_modular.engine import train_step, test_step\n",
        "\n",
        "# Add writer parameter to train()\n",
        "def train(model: torch.nn.Module, \n",
        "          train_dataloader: torch.utils.data.DataLoader, \n",
        "          test_dataloader: torch.utils.data.DataLoader, \n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device, \n",
        "          writer: torch.utils.tensorboard.writer.SummaryWriter # new parameter to take in a writer\n",
        "          ) -> Dict[str, List]:\n",
        "    \"\"\"Trains and tests a PyTorch model.\n",
        "\n",
        "    Passes a target PyTorch models through train_step() and test_step()\n",
        "    functions for a number of epochs, training and testing the model\n",
        "    in the same epoch loop.\n",
        "\n",
        "    Calculates, prints and stores evaluation metrics throughout.\n",
        "\n",
        "    Stores metrics to specified writer log_dir if present.\n",
        "\n",
        "    Args:\n",
        "      model: A PyTorch model to be trained and tested.\n",
        "      train_dataloader: A DataLoader instance for the model to be trained on.\n",
        "      test_dataloader: A DataLoader instance for the model to be tested on.\n",
        "      optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "      loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
        "      epochs: An integer indicating how many epochs to train for.\n",
        "      device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "      writer: A SummaryWriter() instance to log model results to.\n",
        "\n",
        "    Returns:\n",
        "      A dictionary of training and testing loss as well as training and\n",
        "      testing accuracy metrics. Each metric has a value in a list for \n",
        "      each epoch.\n",
        "      In the form: {train_loss: [...],\n",
        "                train_acc: [...],\n",
        "                test_loss: [...],\n",
        "                test_acc: [...]} \n",
        "      For example if training for epochs=2: \n",
        "              {train_loss: [2.0616, 1.0537],\n",
        "                train_acc: [0.3945, 0.3945],\n",
        "                test_loss: [1.2641, 1.5706],\n",
        "                test_acc: [0.3400, 0.2973]} \n",
        "    \"\"\"\n",
        "    # Create empty results dictionary\n",
        "    results = {\"train_loss\": [],\n",
        "               \"train_acc\": [],\n",
        "               \"test_loss\": [],\n",
        "               \"test_acc\": []\n",
        "    }\n",
        "\n",
        "    # Loop through training and testing steps for a number of epochs\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss, train_acc = train_step(model=model,\n",
        "                                          dataloader=train_dataloader,\n",
        "                                          loss_fn=loss_fn,\n",
        "                                          optimizer=optimizer,\n",
        "                                          device=device)\n",
        "        test_loss, test_acc = test_step(model=model,\n",
        "          dataloader=test_dataloader,\n",
        "          loss_fn=loss_fn,\n",
        "          device=device)\n",
        "\n",
        "        # Print out what's happening\n",
        "        print(\n",
        "          f\"Epoch: {epoch+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"train_acc: {train_acc:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "          f\"test_acc: {test_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Update results dictionary\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "\n",
        "        ### New: Use the writer parameter to track experiments ###\n",
        "        # See if there's a writer, if so, log to it\n",
        "        if writer:\n",
        "            # Add results to SummaryWriter\n",
        "            writer.add_scalars(main_tag=\"Loss\", \n",
        "                               tag_scalar_dict={\"train_loss\": train_loss,\n",
        "                                                \"test_loss\": test_loss},\n",
        "                               global_step=epoch)\n",
        "            writer.add_scalars(main_tag=\"Accuracy\", \n",
        "                               tag_scalar_dict={\"train_acc\": train_acc,\n",
        "                                                \"test_acc\": test_acc}, \n",
        "                               global_step=epoch)\n",
        "\n",
        "            # Close the writer\n",
        "            writer.close()\n",
        "        else:\n",
        "            pass\n",
        "    ### End new ###\n",
        "\n",
        "    # Return the filled results at the end of the epochs\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh8jKzHYHYL3"
      },
      "source": [
        "### Download data\n",
        "\n",
        "Using the same data from https://www.learnpytorch.io/07_pytorch_experiment_tracking/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68QGCR_1tzif",
        "outputId": "a5073b19-1463-4d8a-ec08-5399945196a4"
      },
      "outputs": [],
      "source": [
        "# # Download 10 percent and 20 percent training data (if necessary)\n",
        "# data_10_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n",
        "#                                      destination=\"pizza_steak_sushi\")\n",
        "\n",
        "# data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
        "#                                      destination=\"pizza_steak_sushi_20_percent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L2rCRxvt1ED",
        "outputId": "30b8202e-96f3-443f-e89e-2a83e11b1c85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training directory 10%: ..\\data\\pizza_steak_sushi\\train\n",
            "Training directory 20%: ..\\data\\pizza_steak_sushi_20_percent\\train\n",
            "Testing directory: ..\\data\\pizza_steak_sushi\\test\n"
          ]
        }
      ],
      "source": [
        "data_10_percent_path = Path('../data/pizza_steak_sushi')\n",
        "data_20_percent_path = Path('../data/pizza_steak_sushi_20_percent')\n",
        "# Setup training directory paths\n",
        "train_dir_10_percent = data_10_percent_path / \"train\"\n",
        "train_dir_20_percent = data_20_percent_path / \"train\"\n",
        "\n",
        "# Setup testing directory paths (note: use the same test dataset for both to compare the results)\n",
        "test_dir = data_10_percent_path / \"test\"\n",
        "\n",
        "# Check the directories\n",
        "print(f\"Training directory 10%: {train_dir_10_percent}\")\n",
        "print(f\"Training directory 20%: {train_dir_20_percent}\")\n",
        "print(f\"Testing directory: {test_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "K35q9wswt6NH"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Create a transform to normalize data distribution to be inline with ImageNet\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], # values per colour channel [red, green, blue]\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "# Create a transform pipeline\n",
        "simple_transform = transforms.Compose([\n",
        "                                       transforms.Resize((224, 224)),\n",
        "                                       transforms.ToTensor(), # get image values between 0 & 1\n",
        "                                       normalize\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBuEla8pHea9"
      },
      "source": [
        "### Turn data into DataLoaders "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlQU94HBuqOq",
        "outputId": "db4b9144-fd3b-4f31-e0c1-ebd60f906232"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of batches of size 32 in 10 percent training data: 8\n",
            "Number of batches of size 32 in 20 percent training data: 15\n",
            "Number of batches of size 32 in testing data: 8 (all experiments will use the same test set)\n",
            "Number of classes: 3, class names: ['pizza', 'steak', 'sushi']\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 32\n",
        "\n",
        "# Create 10% training and test DataLoaders\n",
        "train_dataloader_10_percent, test_dataloader, class_names = data_loaders.create_dataloaders(train_dir=train_dir_10_percent,\n",
        "                                                                                          test_dir=test_dir,\n",
        "                                                                                          transform=simple_transform,\n",
        "                                                                                          batch_size=BATCH_SIZE)\n",
        "\n",
        "# Create 20% training and test DataLoaders\n",
        "train_dataloader_20_percent, test_dataloader, class_names = data_loaders.create_dataloaders(train_dir=train_dir_20_percent,\n",
        "                                                                                          test_dir=test_dir,\n",
        "                                                                                          transform=simple_transform,\n",
        "                                                                                          batch_size=BATCH_SIZE)\n",
        "\n",
        "# Find the number of samples/batches per dataloader (using the same test_dataloader for both experiments)\n",
        "print(f\"Number of batches of size {BATCH_SIZE} in 10 percent training data: {len(train_dataloader_10_percent)}\")\n",
        "print(f\"Number of batches of size {BATCH_SIZE} in 20 percent training data: {len(train_dataloader_20_percent)}\")\n",
        "print(f\"Number of batches of size {BATCH_SIZE} in testing data: {len(train_dataloader_10_percent)} (all experiments will use the same test set)\")\n",
        "print(f\"Number of classes: {len(class_names)}, class names: {class_names}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwmoMhW8IqSu"
      },
      "source": [
        "## Exercise 1: Pick a larger model from [`torchvision.models`](https://pytorch.org/vision/main/models.html) to add to the list of experiments (for example, EffNetB3 or higher)\n",
        "\n",
        "* How does it perform compared to our existing models?\n",
        "* **Hint:** You'll need to set up an exerpiment similar to [07. PyTorch Experiment Tracking section 7.6](https://www.learnpytorch.io/07_pytorch_experiment_tracking/#76-create-experiments-and-set-up-training-code)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "F-35y0uxJ8tg"
      },
      "outputs": [],
      "source": [
        "# TODO: your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================================================================================================\n",
            "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
            "============================================================================================================================================\n",
            "EfficientNet (EfficientNet)                                  [32, 3, 600, 600]    [32, 1000]           --                   True\n",
            "├─Sequential (features)                                      [32, 3, 600, 600]    [32, 2560, 19, 19]   --                   True\n",
            "│    └─Conv2dNormActivation (0)                              [32, 3, 600, 600]    [32, 64, 300, 300]   --                   True\n",
            "│    │    └─Conv2d (0)                                       [32, 3, 600, 600]    [32, 64, 300, 300]   1,728                True\n",
            "│    │    └─BatchNorm2d (1)                                  [32, 64, 300, 300]   [32, 64, 300, 300]   128                  True\n",
            "│    │    └─SiLU (2)                                         [32, 64, 300, 300]   [32, 64, 300, 300]   --                   --\n",
            "│    └─Sequential (1)                                        [32, 64, 300, 300]   [32, 32, 300, 300]   --                   True\n",
            "│    │    └─MBConv (0)                                       [32, 64, 300, 300]   [32, 32, 300, 300]   4,944                True\n",
            "│    │    └─MBConv (1)                                       [32, 32, 300, 300]   [32, 32, 300, 300]   1,992                True\n",
            "│    │    └─MBConv (2)                                       [32, 32, 300, 300]   [32, 32, 300, 300]   1,992                True\n",
            "│    │    └─MBConv (3)                                       [32, 32, 300, 300]   [32, 32, 300, 300]   1,992                True\n",
            "│    └─Sequential (2)                                        [32, 32, 300, 300]   [32, 48, 150, 150]   --                   True\n",
            "│    │    └─MBConv (0)                                       [32, 32, 300, 300]   [32, 48, 150, 150]   21,224               True\n",
            "│    │    └─MBConv (1)                                       [32, 48, 150, 150]   [32, 48, 150, 150]   38,700               True\n",
            "│    │    └─MBConv (2)                                       [32, 48, 150, 150]   [32, 48, 150, 150]   38,700               True\n",
            "│    │    └─MBConv (3)                                       [32, 48, 150, 150]   [32, 48, 150, 150]   38,700               True\n",
            "│    │    └─MBConv (4)                                       [32, 48, 150, 150]   [32, 48, 150, 150]   38,700               True\n",
            "│    │    └─MBConv (5)                                       [32, 48, 150, 150]   [32, 48, 150, 150]   38,700               True\n",
            "│    │    └─MBConv (6)                                       [32, 48, 150, 150]   [32, 48, 150, 150]   38,700               True\n",
            "│    └─Sequential (3)                                        [32, 48, 150, 150]   [32, 80, 75, 75]     --                   True\n",
            "│    │    └─MBConv (0)                                       [32, 48, 150, 150]   [32, 80, 75, 75]     52,588               True\n",
            "│    │    └─MBConv (1)                                       [32, 80, 75, 75]     [32, 80, 75, 75]     110,580              True\n",
            "│    │    └─MBConv (2)                                       [32, 80, 75, 75]     [32, 80, 75, 75]     110,580              True\n",
            "│    │    └─MBConv (3)                                       [32, 80, 75, 75]     [32, 80, 75, 75]     110,580              True\n",
            "│    │    └─MBConv (4)                                       [32, 80, 75, 75]     [32, 80, 75, 75]     110,580              True\n",
            "│    │    └─MBConv (5)                                       [32, 80, 75, 75]     [32, 80, 75, 75]     110,580              True\n",
            "│    │    └─MBConv (6)                                       [32, 80, 75, 75]     [32, 80, 75, 75]     110,580              True\n",
            "│    └─Sequential (4)                                        [32, 80, 75, 75]     [32, 160, 38, 38]    --                   True\n",
            "│    │    └─MBConv (0)                                       [32, 80, 75, 75]     [32, 160, 38, 38]    141,460              True\n",
            "│    │    └─MBConv (1)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    397,800              True\n",
            "│    │    └─MBConv (2)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    397,800              True\n",
            "│    │    └─MBConv (3)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    397,800              True\n",
            "│    │    └─MBConv (4)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    397,800              True\n",
            "│    │    └─MBConv (5)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    397,800              True\n",
            "│    │    └─MBConv (6)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    397,800              True\n",
            "│    │    └─MBConv (7)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    397,800              True\n",
            "│    │    └─MBConv (8)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    397,800              True\n",
            "│    │    └─MBConv (9)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    397,800              True\n",
            "│    └─Sequential (5)                                        [32, 160, 38, 38]    [32, 224, 38, 38]    --                   True\n",
            "│    │    └─MBConv (0)                                       [32, 160, 38, 38]    [32, 224, 38, 38]    474,728              True\n",
            "│    │    └─MBConv (1)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    793,464              True\n",
            "│    │    └─MBConv (2)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    793,464              True\n",
            "│    │    └─MBConv (3)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    793,464              True\n",
            "│    │    └─MBConv (4)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    793,464              True\n",
            "│    │    └─MBConv (5)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    793,464              True\n",
            "│    │    └─MBConv (6)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    793,464              True\n",
            "│    │    └─MBConv (7)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    793,464              True\n",
            "│    │    └─MBConv (8)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    793,464              True\n",
            "│    │    └─MBConv (9)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    793,464              True\n",
            "│    └─Sequential (6)                                        [32, 224, 38, 38]    [32, 384, 19, 19]    --                   True\n",
            "│    │    └─MBConv (0)                                       [32, 224, 38, 38]    [32, 384, 19, 19]    1,008,824            True\n",
            "│    │    └─MBConv (1)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    2,281,824            True\n",
            "│    │    └─MBConv (2)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    2,281,824            True\n",
            "│    │    └─MBConv (3)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    2,281,824            True\n",
            "│    │    └─MBConv (4)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    2,281,824            True\n",
            "│    │    └─MBConv (5)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    2,281,824            True\n",
            "│    │    └─MBConv (6)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    2,281,824            True\n",
            "│    │    └─MBConv (7)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    2,281,824            True\n",
            "│    │    └─MBConv (8)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    2,281,824            True\n",
            "│    │    └─MBConv (9)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    2,281,824            True\n",
            "│    │    └─MBConv (10)                                      [32, 384, 19, 19]    [32, 384, 19, 19]    2,281,824            True\n",
            "│    │    └─MBConv (11)                                      [32, 384, 19, 19]    [32, 384, 19, 19]    2,281,824            True\n",
            "│    │    └─MBConv (12)                                      [32, 384, 19, 19]    [32, 384, 19, 19]    2,281,824            True\n",
            "│    └─Sequential (7)                                        [32, 384, 19, 19]    [32, 640, 19, 19]    --                   True\n",
            "│    │    └─MBConv (0)                                       [32, 384, 19, 19]    [32, 640, 19, 19]    2,835,296            True\n",
            "│    │    └─MBConv (1)                                       [32, 640, 19, 19]    [32, 640, 19, 19]    6,199,200            True\n",
            "│    │    └─MBConv (2)                                       [32, 640, 19, 19]    [32, 640, 19, 19]    6,199,200            True\n",
            "│    │    └─MBConv (3)                                       [32, 640, 19, 19]    [32, 640, 19, 19]    6,199,200            True\n",
            "│    └─Conv2dNormActivation (8)                              [32, 640, 19, 19]    [32, 2560, 19, 19]   --                   True\n",
            "│    │    └─Conv2d (0)                                       [32, 640, 19, 19]    [32, 2560, 19, 19]   1,638,400            True\n",
            "│    │    └─BatchNorm2d (1)                                  [32, 2560, 19, 19]   [32, 2560, 19, 19]   5,120                True\n",
            "│    │    └─SiLU (2)                                         [32, 2560, 19, 19]   [32, 2560, 19, 19]   --                   --\n",
            "├─AdaptiveAvgPool2d (avgpool)                                [32, 2560, 19, 19]   [32, 2560, 1, 1]     --                   --\n",
            "├─Sequential (classifier)                                    [32, 2560]           [32, 1000]           --                   True\n",
            "│    └─Dropout (0)                                           [32, 2560]           [32, 2560]           --                   --\n",
            "│    └─Linear (1)                                            [32, 2560]           [32, 1000]           2,561,000            True\n",
            "============================================================================================================================================\n",
            "Total params: 66,347,960\n",
            "Trainable params: 66,347,960\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.TERABYTES): 1.21\n",
            "============================================================================================================================================\n",
            "Input size (MB): 138.24\n",
            "Forward/backward pass size (MB): 148430.01\n",
            "Params size (MB): 265.39\n",
            "Estimated Total Size (MB): 148833.64\n",
            "============================================================================================================================================\n",
            " number of input_features of final layter 2560\n"
          ]
        }
      ],
      "source": [
        "# TODO: your code\n",
        "effnetb7_weights = torchvision.models.EfficientNet_B7_Weights.DEFAULT\n",
        "effnetb7 = torchvision.models.efficientnet_b7(weights=effnetb7_weights)\n",
        "\n",
        "print (summary (model= effnetb7,\n",
        "       input_size = (32, 3, 600, 600),\n",
        "       col_names = ['input_size', 'output_size', 'num_params', 'trainable'],\n",
        "       col_width = 20,\n",
        "       row_settings = ['var_names']))\n",
        "\n",
        "print (f' number of input_features of final layter {len (effnetb7.classifier.state_dict()[\"1.weight\"][0])}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "#  2560\n",
        "OUT_FEATURES = len (class_names)\n",
        "def create_effnetb7():\n",
        "    weights = torchvision.models.EfficientNet_B7_Weights.DEFAULT\n",
        "    model = torchvision.models.efficientnet_b7(weights=weights).to(device)\n",
        "\n",
        "    for param in model.features.parameters():\n",
        "        param.requires_grad=False\n",
        "\n",
        "    set_seeds()\n",
        "\n",
        "    model.classifier= nn.Sequential(\n",
        "            nn.Dropout(p=0.2),\n",
        "            nn.Linear(in_features = 2560, out_features = OUT_FEATURES)\n",
        "    ).to(device)\n",
        "\n",
        "    model.name = 'efnenetb7'\n",
        "    print (f'[INFO] created new {model.name} model')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] created new efnenetb7 model\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "============================================================================================================================================\n",
              "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
              "============================================================================================================================================\n",
              "EfficientNet (EfficientNet)                                  [32, 3, 600, 600]    [32, 3]              --                   Partial\n",
              "├─Sequential (features)                                      [32, 3, 600, 600]    [32, 2560, 19, 19]   --                   False\n",
              "│    └─Conv2dNormActivation (0)                              [32, 3, 600, 600]    [32, 64, 300, 300]   --                   False\n",
              "│    │    └─Conv2d (0)                                       [32, 3, 600, 600]    [32, 64, 300, 300]   (1,728)              False\n",
              "│    │    └─BatchNorm2d (1)                                  [32, 64, 300, 300]   [32, 64, 300, 300]   (128)                False\n",
              "│    │    └─SiLU (2)                                         [32, 64, 300, 300]   [32, 64, 300, 300]   --                   --\n",
              "│    └─Sequential (1)                                        [32, 64, 300, 300]   [32, 32, 300, 300]   --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 64, 300, 300]   [32, 32, 300, 300]   (4,944)              False\n",
              "│    │    └─MBConv (1)                                       [32, 32, 300, 300]   [32, 32, 300, 300]   (1,992)              False\n",
              "│    │    └─MBConv (2)                                       [32, 32, 300, 300]   [32, 32, 300, 300]   (1,992)              False\n",
              "│    │    └─MBConv (3)                                       [32, 32, 300, 300]   [32, 32, 300, 300]   (1,992)              False\n",
              "│    └─Sequential (2)                                        [32, 32, 300, 300]   [32, 48, 150, 150]   --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 32, 300, 300]   [32, 48, 150, 150]   (21,224)             False\n",
              "│    │    └─MBConv (1)                                       [32, 48, 150, 150]   [32, 48, 150, 150]   (38,700)             False\n",
              "│    │    └─MBConv (2)                                       [32, 48, 150, 150]   [32, 48, 150, 150]   (38,700)             False\n",
              "│    │    └─MBConv (3)                                       [32, 48, 150, 150]   [32, 48, 150, 150]   (38,700)             False\n",
              "│    │    └─MBConv (4)                                       [32, 48, 150, 150]   [32, 48, 150, 150]   (38,700)             False\n",
              "│    │    └─MBConv (5)                                       [32, 48, 150, 150]   [32, 48, 150, 150]   (38,700)             False\n",
              "│    │    └─MBConv (6)                                       [32, 48, 150, 150]   [32, 48, 150, 150]   (38,700)             False\n",
              "│    └─Sequential (3)                                        [32, 48, 150, 150]   [32, 80, 75, 75]     --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 48, 150, 150]   [32, 80, 75, 75]     (52,588)             False\n",
              "│    │    └─MBConv (1)                                       [32, 80, 75, 75]     [32, 80, 75, 75]     (110,580)            False\n",
              "│    │    └─MBConv (2)                                       [32, 80, 75, 75]     [32, 80, 75, 75]     (110,580)            False\n",
              "│    │    └─MBConv (3)                                       [32, 80, 75, 75]     [32, 80, 75, 75]     (110,580)            False\n",
              "│    │    └─MBConv (4)                                       [32, 80, 75, 75]     [32, 80, 75, 75]     (110,580)            False\n",
              "│    │    └─MBConv (5)                                       [32, 80, 75, 75]     [32, 80, 75, 75]     (110,580)            False\n",
              "│    │    └─MBConv (6)                                       [32, 80, 75, 75]     [32, 80, 75, 75]     (110,580)            False\n",
              "│    └─Sequential (4)                                        [32, 80, 75, 75]     [32, 160, 38, 38]    --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 80, 75, 75]     [32, 160, 38, 38]    (141,460)            False\n",
              "│    │    └─MBConv (1)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    (397,800)            False\n",
              "│    │    └─MBConv (2)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    (397,800)            False\n",
              "│    │    └─MBConv (3)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    (397,800)            False\n",
              "│    │    └─MBConv (4)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    (397,800)            False\n",
              "│    │    └─MBConv (5)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    (397,800)            False\n",
              "│    │    └─MBConv (6)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    (397,800)            False\n",
              "│    │    └─MBConv (7)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    (397,800)            False\n",
              "│    │    └─MBConv (8)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    (397,800)            False\n",
              "│    │    └─MBConv (9)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    (397,800)            False\n",
              "│    └─Sequential (5)                                        [32, 160, 38, 38]    [32, 224, 38, 38]    --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 160, 38, 38]    [32, 224, 38, 38]    (474,728)            False\n",
              "│    │    └─MBConv (1)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    (793,464)            False\n",
              "│    │    └─MBConv (2)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    (793,464)            False\n",
              "│    │    └─MBConv (3)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    (793,464)            False\n",
              "│    │    └─MBConv (4)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    (793,464)            False\n",
              "│    │    └─MBConv (5)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    (793,464)            False\n",
              "│    │    └─MBConv (6)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    (793,464)            False\n",
              "│    │    └─MBConv (7)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    (793,464)            False\n",
              "│    │    └─MBConv (8)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    (793,464)            False\n",
              "│    │    └─MBConv (9)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    (793,464)            False\n",
              "│    └─Sequential (6)                                        [32, 224, 38, 38]    [32, 384, 19, 19]    --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 224, 38, 38]    [32, 384, 19, 19]    (1,008,824)          False\n",
              "│    │    └─MBConv (1)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
              "│    │    └─MBConv (2)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
              "│    │    └─MBConv (3)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
              "│    │    └─MBConv (4)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
              "│    │    └─MBConv (5)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
              "│    │    └─MBConv (6)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
              "│    │    └─MBConv (7)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
              "│    │    └─MBConv (8)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
              "│    │    └─MBConv (9)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
              "│    │    └─MBConv (10)                                      [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
              "│    │    └─MBConv (11)                                      [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
              "│    │    └─MBConv (12)                                      [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
              "│    └─Sequential (7)                                        [32, 384, 19, 19]    [32, 640, 19, 19]    --                   False\n",
              "│    │    └─MBConv (0)                                       [32, 384, 19, 19]    [32, 640, 19, 19]    (2,835,296)          False\n",
              "│    │    └─MBConv (1)                                       [32, 640, 19, 19]    [32, 640, 19, 19]    (6,199,200)          False\n",
              "│    │    └─MBConv (2)                                       [32, 640, 19, 19]    [32, 640, 19, 19]    (6,199,200)          False\n",
              "│    │    └─MBConv (3)                                       [32, 640, 19, 19]    [32, 640, 19, 19]    (6,199,200)          False\n",
              "│    └─Conv2dNormActivation (8)                              [32, 640, 19, 19]    [32, 2560, 19, 19]   --                   False\n",
              "│    │    └─Conv2d (0)                                       [32, 640, 19, 19]    [32, 2560, 19, 19]   (1,638,400)          False\n",
              "│    │    └─BatchNorm2d (1)                                  [32, 2560, 19, 19]   [32, 2560, 19, 19]   (5,120)              False\n",
              "│    │    └─SiLU (2)                                         [32, 2560, 19, 19]   [32, 2560, 19, 19]   --                   --\n",
              "├─AdaptiveAvgPool2d (avgpool)                                [32, 2560, 19, 19]   [32, 2560, 1, 1]     --                   --\n",
              "├─Sequential (classifier)                                    [32, 2560]           [32, 3]              --                   True\n",
              "│    └─Dropout (0)                                           [32, 2560]           [32, 2560]           --                   --\n",
              "│    └─Linear (1)                                            [32, 2560]           [32, 3]              7,683                True\n",
              "============================================================================================================================================\n",
              "Total params: 63,794,643\n",
              "Trainable params: 7,683\n",
              "Non-trainable params: 63,786,960\n",
              "Total mult-adds (Units.TERABYTES): 1.21\n",
              "============================================================================================================================================\n",
              "Input size (MB): 138.24\n",
              "Forward/backward pass size (MB): 148429.76\n",
              "Params size (MB): 255.18\n",
              "Estimated Total Size (MB): 148823.17\n",
              "============================================================================================================================================"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "effnetb7 = create_effnetb7()\n",
        "\n",
        "summary(model = effnetb7,\n",
        "         input_size = (32, 3, 600, 600),\n",
        "         col_names=['input_size', 'output_size', 'num_params', 'trainable'],\n",
        "         col_width = 20,\n",
        "         row_settings = ['var_names'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_epochs = [5, 10]\n",
        "models = ['effnetb7']\n",
        "train_dataloaders = {'data_10_percent': train_dataloader_10_percent,\n",
        "                     'data_20_percent':train_dataloader_20_percent}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment no: 1\n",
            "Model: effnetb7\n",
            "Dataloader: data_10_percent\n",
            "number of epochs: 5\n",
            "[INFO] created new efnenetb7 model\n",
            "[INFO] Created SummaryWriter, saving to: runs\\2024-09-22\\data_10_percent\\effnetb7\\5_epochs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 1/5 [00:05<00:22,  5.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 | train_loss: 1.0192 | train_acc: 0.6094 | test_loss: 0.9639 | test_acc: 0.6402\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 2/5 [00:08<00:12,  4.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2 | train_loss: 0.8261 | train_acc: 0.8398 | test_loss: 0.8761 | test_acc: 0.6515\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 3/5 [00:12<00:07,  3.78s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3 | train_loss: 0.7251 | train_acc: 0.8594 | test_loss: 0.7926 | test_acc: 0.6932\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 4/5 [00:15<00:03,  3.48s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 4 | train_loss: 0.7154 | train_acc: 0.7891 | test_loss: 0.7136 | test_acc: 0.7443\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:18<00:00,  3.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 5 | train_loss: 0.5877 | train_acc: 0.8281 | test_loss: 0.6531 | test_acc: 0.7955\n",
            "[INFO] Saving model to: models\\07ex_effnetb7_data_10_percent_5_epochs.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__________________________________________________\n",
            "\n",
            "Experiment no: 2\n",
            "Model: effnetb7\n",
            "Dataloader: data_10_percent\n",
            "number of epochs: 10\n",
            "[INFO] created new efnenetb7 model\n",
            "[INFO] Created SummaryWriter, saving to: runs\\2024-09-22\\data_10_percent\\effnetb7\\10_epochs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 1/10 [00:03<00:30,  3.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 | train_loss: 1.0192 | train_acc: 0.6094 | test_loss: 0.9639 | test_acc: 0.6402\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 2/10 [00:06<00:27,  3.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2 | train_loss: 0.8261 | train_acc: 0.8398 | test_loss: 0.8761 | test_acc: 0.6515\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 3/10 [00:10<00:23,  3.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3 | train_loss: 0.7251 | train_acc: 0.8594 | test_loss: 0.7926 | test_acc: 0.6932\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 4/10 [00:13<00:20,  3.44s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 4 | train_loss: 0.7154 | train_acc: 0.7891 | test_loss: 0.7136 | test_acc: 0.7443\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 5/10 [00:17<00:17,  3.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 5 | train_loss: 0.5877 | train_acc: 0.8281 | test_loss: 0.6531 | test_acc: 0.7955\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 6/10 [00:20<00:13,  3.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 6 | train_loss: 0.5404 | train_acc: 0.8320 | test_loss: 0.6005 | test_acc: 0.8163\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 7/10 [00:23<00:10,  3.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 7 | train_loss: 0.4993 | train_acc: 0.8477 | test_loss: 0.5570 | test_acc: 0.8059\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 8/10 [00:27<00:06,  3.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 8 | train_loss: 0.4658 | train_acc: 0.9688 | test_loss: 0.5364 | test_acc: 0.8059\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 9/10 [00:30<00:03,  3.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 9 | train_loss: 0.4606 | train_acc: 0.8438 | test_loss: 0.5224 | test_acc: 0.8059\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:33<00:00,  3.35s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10 | train_loss: 0.3910 | train_acc: 0.9688 | test_loss: 0.5040 | test_acc: 0.8059\n",
            "[INFO] Saving model to: models\\07ex_effnetb7_data_10_percent_10_epochs.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__________________________________________________\n",
            "\n",
            "Experiment no: 3\n",
            "Model: effnetb7\n",
            "Dataloader: data_20_percent\n",
            "number of epochs: 5\n",
            "[INFO] created new efnenetb7 model\n",
            "[INFO] Created SummaryWriter, saving to: runs\\2024-09-22\\data_20_percent\\effnetb7\\5_epochs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 1/5 [00:07<00:29,  7.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 | train_loss: 0.9590 | train_acc: 0.6604 | test_loss: 0.8629 | test_acc: 0.7443\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 2/5 [00:12<00:18,  6.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2 | train_loss: 0.6992 | train_acc: 0.8625 | test_loss: 0.6864 | test_acc: 0.8466\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 3/5 [00:17<00:11,  5.56s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3 | train_loss: 0.5729 | train_acc: 0.8917 | test_loss: 0.5823 | test_acc: 0.8570\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 4/5 [00:22<00:05,  5.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 4 | train_loss: 0.4894 | train_acc: 0.8625 | test_loss: 0.5209 | test_acc: 0.8371\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:27<00:00,  5.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 5 | train_loss: 0.4465 | train_acc: 0.9104 | test_loss: 0.4860 | test_acc: 0.8371\n",
            "[INFO] Saving model to: models\\07ex_effnetb7_data_20_percent_5_epochs.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__________________________________________________\n",
            "\n",
            "Experiment no: 4\n",
            "Model: effnetb7\n",
            "Dataloader: data_20_percent\n",
            "number of epochs: 10\n",
            "[INFO] created new efnenetb7 model\n",
            "[INFO] Created SummaryWriter, saving to: runs\\2024-09-22\\data_20_percent\\effnetb7\\10_epochs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 1/10 [00:05<00:46,  5.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 | train_loss: 0.9590 | train_acc: 0.6604 | test_loss: 0.8629 | test_acc: 0.7443\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 2/10 [00:09<00:39,  4.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2 | train_loss: 0.6992 | train_acc: 0.8625 | test_loss: 0.6864 | test_acc: 0.8466\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 3/10 [00:15<00:37,  5.29s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3 | train_loss: 0.5729 | train_acc: 0.8917 | test_loss: 0.5823 | test_acc: 0.8570\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 4/10 [00:21<00:32,  5.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 4 | train_loss: 0.4894 | train_acc: 0.8625 | test_loss: 0.5209 | test_acc: 0.8371\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 5/10 [00:26<00:27,  5.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 5 | train_loss: 0.4465 | train_acc: 0.9104 | test_loss: 0.4860 | test_acc: 0.8371\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 6/10 [00:32<00:22,  5.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 6 | train_loss: 0.4158 | train_acc: 0.9083 | test_loss: 0.4666 | test_acc: 0.8267\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 7/10 [00:37<00:16,  5.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 7 | train_loss: 0.3705 | train_acc: 0.9229 | test_loss: 0.4546 | test_acc: 0.8267\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 8/10 [00:43<00:10,  5.42s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 8 | train_loss: 0.3514 | train_acc: 0.9042 | test_loss: 0.4509 | test_acc: 0.8466\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 9/10 [00:48<00:05,  5.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 9 | train_loss: 0.3196 | train_acc: 0.9292 | test_loss: 0.4471 | test_acc: 0.8466\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:54<00:00,  5.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 10 | train_loss: 0.3169 | train_acc: 0.9125 | test_loss: 0.4383 | test_acc: 0.8059\n",
            "[INFO] Saving model to: models\\07ex_effnetb7_data_20_percent_10_epochs.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "__________________________________________________\n",
            "\n",
            "CPU times: total: 14min 29s\n",
            "Wall time: 2min 19s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from going_modular.utils import save_model\n",
        "set_seeds (seed=42)\n",
        "experiment_num = 0\n",
        "\n",
        "for dataloader_name, train_data_loader in train_dataloaders.items():\n",
        "    for epochs in num_epochs:\n",
        "        for model_name in models:\n",
        "            experiment_num +=1\n",
        "            print(f'Experiment no: {experiment_num}')\n",
        "            print(f'Model: {model_name}')\n",
        "            print(f'Dataloader: {dataloader_name}')\n",
        "            print (f'number of epochs: {epochs}')\n",
        "\n",
        "            if model_name == 'effnetb7':\n",
        "                model = create_effnetb7()\n",
        "            else:\n",
        "                print ('no model found')\n",
        "                break\n",
        "\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "            optimizer = torch.optim.Adam(params=model.parameters(), lr = 0.001)\n",
        "\n",
        "            train (model = model,\n",
        "                train_dataloader=train_data_loader,\n",
        "                test_dataloader = test_dataloader,\n",
        "                optimizer = optimizer,\n",
        "                loss_fn = loss_fn,\n",
        "                epochs = epochs,\n",
        "                device = device,\n",
        "                writer= create_writer(experiment_name = dataloader_name,\n",
        "                    model_name = model_name,\n",
        "                    extra = f'{epochs}_epochs'))\n",
        "            save_filepath = f'07ex_{model_name}_{dataloader_name}_{epochs}_epochs.pth'\n",
        "            save_model (model = model,\n",
        "                target_dir = 'models',\n",
        "                model_name = save_filepath)\n",
        "\n",
        "            print ('_'*50 + '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 87220), started 4 days, 19:25:09 ago. (Use '!kill 87220' to kill it.)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-4b7135d2742d014c\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-4b7135d2742d014c\");\n",
              "          const url = new URL(\"http://localhost\");\n",
              "          const port = 6006;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqlStPo-gbrF"
      },
      "source": [
        "## Exercise 2. Introduce data augmentation to the list of experiments using the 20% pizza, steak, sushi training and test datasets, does this change anything?\n",
        "    \n",
        "* For example, you could have one training DataLoader that uses data augmentation (e.g. `train_dataloader_20_percent_aug` and `train_dataloader_20_percent_no_aug`) and then compare the results of two of the same model types training on these two DataLoaders.\n",
        "* **Note:** You may need to alter the `create_dataloaders()` function to be able to take a transform for the training data and the testing data (because you don't need to perform data augmentation on the test data). See [04. PyTorch Custom Datasets section 6](https://www.learnpytorch.io/04_pytorch_custom_datasets/#6-other-forms-of-transforms-data-augmentation) for examples of using data augmentation or the script below for an example:\n",
        "\n",
        "```python\n",
        "# Note: Data augmentation transform like this should only be performed on training data\n",
        "train_transform_data_aug = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.TrivialAugmentWide(),\n",
        "    transforms.ToTensor(),\n",
        "    normalize\n",
        "])\n",
        "\n",
        "# Create a helper function to visualize different augmented (and not augmented) images\n",
        "def view_dataloader_images(dataloader, n=10):\n",
        "    if n > 10:\n",
        "        print(f\"Having n higher than 10 will create messy plots, lowering to 10.\")\n",
        "        n = 10\n",
        "    imgs, labels = next(iter(dataloader))\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    for i in range(n):\n",
        "        # Min max scale the image for display purposes\n",
        "        targ_image = imgs[i]\n",
        "        sample_min, sample_max = targ_image.min(), targ_image.max()\n",
        "        sample_scaled = (targ_image - sample_min)/(sample_max - sample_min)\n",
        "\n",
        "        # Plot images with appropriate axes information\n",
        "        plt.subplot(1, 10, i+1)\n",
        "        plt.imshow(sample_scaled.permute(1, 2, 0)) # resize for Matplotlib requirements\n",
        "        plt.title(class_names[labels[i]])\n",
        "        plt.axis(False)\n",
        "\n",
        "# Have to update `create_dataloaders()` to handle different augmentations\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "\n",
        "NUM_WORKERS = os.cpu_count() # use maximum number of CPUs for workers to load data \n",
        "\n",
        "# Note: this is an update version of data_setup.create_dataloaders to handle\n",
        "# differnt train and test transforms.\n",
        "def create_dataloaders(\n",
        "    train_dir, \n",
        "    test_dir, \n",
        "    train_transform, # add parameter for train transform (transforms on train dataset)\n",
        "    test_transform,  # add parameter for test transform (transforms on test dataset)\n",
        "    batch_size=32, num_workers=NUM_WORKERS\n",
        "):\n",
        "    # Use ImageFolder to create dataset(s)\n",
        "    train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "    test_data = datasets.ImageFolder(test_dir, transform=test_transform)\n",
        "\n",
        "    # Get class names\n",
        "    class_names = train_data.classes\n",
        "\n",
        "    # Turn images into data loaders\n",
        "    train_dataloader = DataLoader(\n",
        "        train_data,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    test_dataloader = DataLoader(\n",
        "        test_data,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    return train_dataloader, test_dataloader, class_names\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "E1N3yyDOoH2t"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================================================================================================\n",
            "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
            "============================================================================================================================================\n",
            "EfficientNet (EfficientNet)                                  [32, 3, 600, 600]    [32, 3]              --                   Partial\n",
            "├─Sequential (features)                                      [32, 3, 600, 600]    [32, 2560, 19, 19]   --                   False\n",
            "│    └─Conv2dNormActivation (0)                              [32, 3, 600, 600]    [32, 64, 300, 300]   --                   False\n",
            "│    │    └─Conv2d (0)                                       [32, 3, 600, 600]    [32, 64, 300, 300]   (1,728)              False\n",
            "│    │    └─BatchNorm2d (1)                                  [32, 64, 300, 300]   [32, 64, 300, 300]   (128)                False\n",
            "│    │    └─SiLU (2)                                         [32, 64, 300, 300]   [32, 64, 300, 300]   --                   --\n",
            "│    └─Sequential (1)                                        [32, 64, 300, 300]   [32, 32, 300, 300]   --                   False\n",
            "│    │    └─MBConv (0)                                       [32, 64, 300, 300]   [32, 32, 300, 300]   (4,944)              False\n",
            "│    │    └─MBConv (1)                                       [32, 32, 300, 300]   [32, 32, 300, 300]   (1,992)              False\n",
            "│    │    └─MBConv (2)                                       [32, 32, 300, 300]   [32, 32, 300, 300]   (1,992)              False\n",
            "│    │    └─MBConv (3)                                       [32, 32, 300, 300]   [32, 32, 300, 300]   (1,992)              False\n",
            "│    └─Sequential (2)                                        [32, 32, 300, 300]   [32, 48, 150, 150]   --                   False\n",
            "│    │    └─MBConv (0)                                       [32, 32, 300, 300]   [32, 48, 150, 150]   (21,224)             False\n",
            "│    │    └─MBConv (1)                                       [32, 48, 150, 150]   [32, 48, 150, 150]   (38,700)             False\n",
            "│    │    └─MBConv (2)                                       [32, 48, 150, 150]   [32, 48, 150, 150]   (38,700)             False\n",
            "│    │    └─MBConv (3)                                       [32, 48, 150, 150]   [32, 48, 150, 150]   (38,700)             False\n",
            "│    │    └─MBConv (4)                                       [32, 48, 150, 150]   [32, 48, 150, 150]   (38,700)             False\n",
            "│    │    └─MBConv (5)                                       [32, 48, 150, 150]   [32, 48, 150, 150]   (38,700)             False\n",
            "│    │    └─MBConv (6)                                       [32, 48, 150, 150]   [32, 48, 150, 150]   (38,700)             False\n",
            "│    └─Sequential (3)                                        [32, 48, 150, 150]   [32, 80, 75, 75]     --                   False\n",
            "│    │    └─MBConv (0)                                       [32, 48, 150, 150]   [32, 80, 75, 75]     (52,588)             False\n",
            "│    │    └─MBConv (1)                                       [32, 80, 75, 75]     [32, 80, 75, 75]     (110,580)            False\n",
            "│    │    └─MBConv (2)                                       [32, 80, 75, 75]     [32, 80, 75, 75]     (110,580)            False\n",
            "│    │    └─MBConv (3)                                       [32, 80, 75, 75]     [32, 80, 75, 75]     (110,580)            False\n",
            "│    │    └─MBConv (4)                                       [32, 80, 75, 75]     [32, 80, 75, 75]     (110,580)            False\n",
            "│    │    └─MBConv (5)                                       [32, 80, 75, 75]     [32, 80, 75, 75]     (110,580)            False\n",
            "│    │    └─MBConv (6)                                       [32, 80, 75, 75]     [32, 80, 75, 75]     (110,580)            False\n",
            "│    └─Sequential (4)                                        [32, 80, 75, 75]     [32, 160, 38, 38]    --                   False\n",
            "│    │    └─MBConv (0)                                       [32, 80, 75, 75]     [32, 160, 38, 38]    (141,460)            False\n",
            "│    │    └─MBConv (1)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    (397,800)            False\n",
            "│    │    └─MBConv (2)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    (397,800)            False\n",
            "│    │    └─MBConv (3)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    (397,800)            False\n",
            "│    │    └─MBConv (4)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    (397,800)            False\n",
            "│    │    └─MBConv (5)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    (397,800)            False\n",
            "│    │    └─MBConv (6)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    (397,800)            False\n",
            "│    │    └─MBConv (7)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    (397,800)            False\n",
            "│    │    └─MBConv (8)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    (397,800)            False\n",
            "│    │    └─MBConv (9)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    (397,800)            False\n",
            "│    └─Sequential (5)                                        [32, 160, 38, 38]    [32, 224, 38, 38]    --                   False\n",
            "│    │    └─MBConv (0)                                       [32, 160, 38, 38]    [32, 224, 38, 38]    (474,728)            False\n",
            "│    │    └─MBConv (1)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    (793,464)            False\n",
            "│    │    └─MBConv (2)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    (793,464)            False\n",
            "│    │    └─MBConv (3)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    (793,464)            False\n",
            "│    │    └─MBConv (4)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    (793,464)            False\n",
            "│    │    └─MBConv (5)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    (793,464)            False\n",
            "│    │    └─MBConv (6)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    (793,464)            False\n",
            "│    │    └─MBConv (7)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    (793,464)            False\n",
            "│    │    └─MBConv (8)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    (793,464)            False\n",
            "│    │    └─MBConv (9)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    (793,464)            False\n",
            "│    └─Sequential (6)                                        [32, 224, 38, 38]    [32, 384, 19, 19]    --                   False\n",
            "│    │    └─MBConv (0)                                       [32, 224, 38, 38]    [32, 384, 19, 19]    (1,008,824)          False\n",
            "│    │    └─MBConv (1)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
            "│    │    └─MBConv (2)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
            "│    │    └─MBConv (3)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
            "│    │    └─MBConv (4)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
            "│    │    └─MBConv (5)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
            "│    │    └─MBConv (6)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
            "│    │    └─MBConv (7)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
            "│    │    └─MBConv (8)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
            "│    │    └─MBConv (9)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
            "│    │    └─MBConv (10)                                      [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
            "│    │    └─MBConv (11)                                      [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
            "│    │    └─MBConv (12)                                      [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
            "│    └─Sequential (7)                                        [32, 384, 19, 19]    [32, 640, 19, 19]    --                   False\n",
            "│    │    └─MBConv (0)                                       [32, 384, 19, 19]    [32, 640, 19, 19]    (2,835,296)          False\n",
            "│    │    └─MBConv (1)                                       [32, 640, 19, 19]    [32, 640, 19, 19]    (6,199,200)          False\n",
            "│    │    └─MBConv (2)                                       [32, 640, 19, 19]    [32, 640, 19, 19]    (6,199,200)          False\n",
            "│    │    └─MBConv (3)                                       [32, 640, 19, 19]    [32, 640, 19, 19]    (6,199,200)          False\n",
            "│    └─Conv2dNormActivation (8)                              [32, 640, 19, 19]    [32, 2560, 19, 19]   --                   False\n",
            "│    │    └─Conv2d (0)                                       [32, 640, 19, 19]    [32, 2560, 19, 19]   (1,638,400)          False\n",
            "│    │    └─BatchNorm2d (1)                                  [32, 2560, 19, 19]   [32, 2560, 19, 19]   (5,120)              False\n",
            "│    │    └─SiLU (2)                                         [32, 2560, 19, 19]   [32, 2560, 19, 19]   --                   --\n",
            "├─AdaptiveAvgPool2d (avgpool)                                [32, 2560, 19, 19]   [32, 2560, 1, 1]     --                   --\n",
            "├─Sequential (classifier)                                    [32, 2560]           [32, 3]              --                   True\n",
            "│    └─Dropout (0)                                           [32, 2560]           [32, 2560]           --                   --\n",
            "│    └─Linear (1)                                            [32, 2560]           [32, 3]              7,683                True\n",
            "============================================================================================================================================\n",
            "Total params: 63,794,643\n",
            "Trainable params: 7,683\n",
            "Non-trainable params: 63,786,960\n",
            "Total mult-adds (Units.TERABYTES): 1.21\n",
            "============================================================================================================================================\n",
            "Input size (MB): 138.24\n",
            "Forward/backward pass size (MB): 148429.76\n",
            "Params size (MB): 255.18\n",
            "Estimated Total Size (MB): 148823.17\n",
            "============================================================================================================================================\n",
            " number of input_features of final layter 2560\n"
          ]
        }
      ],
      "source": [
        "# TODO: your code\n",
        "# effnetb7_weights = torchvision.models.EfficientNet_B7_Weights.DEFAULT\n",
        "# effnetb7 = torchvision.models.efficientnet_b7(weights=effnetb7_weights)\n",
        "\n",
        "print (summary (model= effnetb7,\n",
        "       input_size = (32, 3, 600, 600),\n",
        "       col_names = ['input_size', 'output_size', 'num_params', 'trainable'],\n",
        "       col_width = 20,\n",
        "       row_settings = ['var_names']))\n",
        "\n",
        "print (f' number of input_features of final layter {len (effnetb7.classifier.state_dict()[\"1.weight\"][0])}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IvuTskxgjaw"
      },
      "source": [
        "## Exercise 3. Scale up the dataset to turn FoodVision Mini into FoodVision Big using the entire [Food101 dataset from `torchvision.models`](https://pytorch.org/vision/stable/generated/torchvision.datasets.Food101.html#torchvision.datasets.Food101)\n",
        "    \n",
        "* You could take the best performing model from your various experiments or even the EffNetB2 feature extractor we created in this notebook and see how it goes fitting for 5 epochs on all of Food101.\n",
        "* If you try more than one model, it would be good to have the model's results tracked.\n",
        "* If you load the Food101 dataset from `torchvision.models`, you'll have to create PyTorch DataLoaders to use it in training.\n",
        "* **Note:** Due to the larger amount of data in Food101 compared to our pizza, steak, sushi dataset, this model will take longer to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YehliYnYoP1x"
      },
      "outputs": [],
      "source": [
        "# TODO: your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## making my own dataloaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Read the Metadata Files\n",
        "Load the metadata files to get the list of image paths for both training and testing datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from shutil import copy2\n",
        "\n",
        "# Paths to metadata files\n",
        "train_metadata_path = '../data/food-101/meta/train.txt'\n",
        "test_metadata_path = '../data/food-101/meta/test.txt'\n",
        "\n",
        "# Function to read metadata files and return list of paths\n",
        "def get_image_paths(metadata_file):\n",
        "    with open(metadata_file, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        return [line.strip() + '.jpg' for line in lines]\n",
        "\n",
        "train_images = get_image_paths(train_metadata_path)\n",
        "test_images = get_image_paths(test_metadata_path)\n",
        "# train_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Train and Test Folders\n",
        "To organize the images into train and test folders, you can create directories for each class within the train and test folders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create directories for train and test datasets\n",
        "def create_dataset_folders(base_path, image_list):\n",
        "    for image in image_list:\n",
        "        class_name = image.split('/')[0]\n",
        "        class_folder = os.path.join(base_path, class_name)\n",
        "        if not os.path.exists(class_folder):\n",
        "            os.makedirs(class_folder)\n",
        "\n",
        "# Base paths for new dataset folders\n",
        "train_base_path = '../data/food-101/train'\n",
        "test_base_path = '../data/food-101/test'\n",
        "\n",
        "create_dataset_folders(train_base_path, train_images)\n",
        "create_dataset_folders(test_base_path, test_images)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Copy Images to Train and Test Folders\n",
        "Now, you can copy the images from the images folder to the respective train and test folders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Original images path\n",
        "# # do this only once\n",
        "images_path = '../data/food-101/images'\n",
        "\n",
        "# # Function to copy images to the appropriate folder\n",
        "# def copy_images(image_list, destination_folder):\n",
        "#     for image in image_list:\n",
        "#         source = os.path.join(images_path, image)\n",
        "#         destination = os.path.join(destination_folder, image)\n",
        "#         copy2(source, destination)\n",
        "\n",
        "# # Copy images to train and test folders\n",
        "# copy_images(train_images, train_base_path)\n",
        "# copy_images(test_images, test_base_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Check Dataset Distribution\n",
        "You can check the distribution of images in the train and test folders to ensure the splits are as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Dataset Distribution:\n",
            "Class: apple_pie, Images: 750\n",
            "Class: baby_back_ribs, Images: 750\n",
            "Class: baklava, Images: 750\n",
            "Class: beef_carpaccio, Images: 750\n",
            "Class: beef_tartare, Images: 750\n",
            "Class: beet_salad, Images: 750\n",
            "Class: beignets, Images: 750\n",
            "Class: bibimbap, Images: 750\n",
            "Class: bread_pudding, Images: 750\n",
            "Class: breakfast_burrito, Images: 750\n",
            "Class: bruschetta, Images: 750\n",
            "Class: caesar_salad, Images: 750\n",
            "Class: cannoli, Images: 750\n",
            "Class: caprese_salad, Images: 750\n",
            "Class: carrot_cake, Images: 750\n",
            "Class: ceviche, Images: 750\n",
            "Class: cheesecake, Images: 750\n",
            "Class: cheese_plate, Images: 750\n",
            "Class: chicken_curry, Images: 750\n",
            "Class: chicken_quesadilla, Images: 750\n",
            "Class: chicken_wings, Images: 750\n",
            "Class: chocolate_cake, Images: 750\n",
            "Class: chocolate_mousse, Images: 750\n",
            "Class: churros, Images: 750\n",
            "Class: clam_chowder, Images: 750\n",
            "Class: club_sandwich, Images: 750\n",
            "Class: crab_cakes, Images: 750\n",
            "Class: creme_brulee, Images: 750\n",
            "Class: croque_madame, Images: 750\n",
            "Class: cup_cakes, Images: 750\n",
            "Class: deviled_eggs, Images: 750\n",
            "Class: donuts, Images: 750\n",
            "Class: dumplings, Images: 750\n",
            "Class: edamame, Images: 750\n",
            "Class: eggs_benedict, Images: 750\n",
            "Class: escargots, Images: 750\n",
            "Class: falafel, Images: 750\n",
            "Class: filet_mignon, Images: 750\n",
            "Class: fish_and_chips, Images: 750\n",
            "Class: foie_gras, Images: 750\n",
            "Class: french_fries, Images: 750\n",
            "Class: french_onion_soup, Images: 750\n",
            "Class: french_toast, Images: 750\n",
            "Class: fried_calamari, Images: 750\n",
            "Class: fried_rice, Images: 750\n",
            "Class: frozen_yogurt, Images: 750\n",
            "Class: garlic_bread, Images: 750\n",
            "Class: gnocchi, Images: 750\n",
            "Class: greek_salad, Images: 750\n",
            "Class: grilled_cheese_sandwich, Images: 750\n",
            "Class: grilled_salmon, Images: 750\n",
            "Class: guacamole, Images: 750\n",
            "Class: gyoza, Images: 750\n",
            "Class: hamburger, Images: 750\n",
            "Class: hot_and_sour_soup, Images: 750\n",
            "Class: hot_dog, Images: 750\n",
            "Class: huevos_rancheros, Images: 750\n",
            "Class: hummus, Images: 750\n",
            "Class: ice_cream, Images: 750\n",
            "Class: lasagna, Images: 750\n",
            "Class: lobster_bisque, Images: 750\n",
            "Class: lobster_roll_sandwich, Images: 750\n",
            "Class: macaroni_and_cheese, Images: 750\n",
            "Class: macarons, Images: 750\n",
            "Class: miso_soup, Images: 750\n",
            "Class: mussels, Images: 750\n",
            "Class: nachos, Images: 750\n",
            "Class: omelette, Images: 750\n",
            "Class: onion_rings, Images: 750\n",
            "Class: oysters, Images: 750\n",
            "Class: pad_thai, Images: 750\n",
            "Class: paella, Images: 750\n",
            "Class: pancakes, Images: 750\n",
            "Class: panna_cotta, Images: 750\n",
            "Class: peking_duck, Images: 750\n",
            "Class: pho, Images: 750\n",
            "Class: pizza, Images: 750\n",
            "Class: pork_chop, Images: 750\n",
            "Class: poutine, Images: 750\n",
            "Class: prime_rib, Images: 750\n",
            "Class: pulled_pork_sandwich, Images: 750\n",
            "Class: ramen, Images: 750\n",
            "Class: ravioli, Images: 750\n",
            "Class: red_velvet_cake, Images: 750\n",
            "Class: risotto, Images: 750\n",
            "Class: samosa, Images: 750\n",
            "Class: sashimi, Images: 750\n",
            "Class: scallops, Images: 750\n",
            "Class: seaweed_salad, Images: 750\n",
            "Class: shrimp_and_grits, Images: 750\n",
            "Class: spaghetti_bolognese, Images: 750\n",
            "Class: spaghetti_carbonara, Images: 750\n",
            "Class: spring_rolls, Images: 750\n",
            "Class: steak, Images: 750\n",
            "Class: strawberry_shortcake, Images: 750\n",
            "Class: sushi, Images: 750\n",
            "Class: tacos, Images: 750\n",
            "Class: takoyaki, Images: 750\n",
            "Class: tiramisu, Images: 750\n",
            "Class: tuna_tartare, Images: 750\n",
            "Class: waffles, Images: 750\n",
            "\n",
            "Test Dataset Distribution:\n",
            "Class: apple_pie, Images: 250\n",
            "Class: baby_back_ribs, Images: 250\n",
            "Class: baklava, Images: 250\n",
            "Class: beef_carpaccio, Images: 250\n",
            "Class: beef_tartare, Images: 250\n",
            "Class: beet_salad, Images: 250\n",
            "Class: beignets, Images: 250\n",
            "Class: bibimbap, Images: 250\n",
            "Class: bread_pudding, Images: 250\n",
            "Class: breakfast_burrito, Images: 250\n",
            "Class: bruschetta, Images: 250\n",
            "Class: caesar_salad, Images: 250\n",
            "Class: cannoli, Images: 250\n",
            "Class: caprese_salad, Images: 250\n",
            "Class: carrot_cake, Images: 250\n",
            "Class: ceviche, Images: 250\n",
            "Class: cheesecake, Images: 250\n",
            "Class: cheese_plate, Images: 250\n",
            "Class: chicken_curry, Images: 250\n",
            "Class: chicken_quesadilla, Images: 250\n",
            "Class: chicken_wings, Images: 250\n",
            "Class: chocolate_cake, Images: 250\n",
            "Class: chocolate_mousse, Images: 250\n",
            "Class: churros, Images: 250\n",
            "Class: clam_chowder, Images: 250\n",
            "Class: club_sandwich, Images: 250\n",
            "Class: crab_cakes, Images: 250\n",
            "Class: creme_brulee, Images: 250\n",
            "Class: croque_madame, Images: 250\n",
            "Class: cup_cakes, Images: 250\n",
            "Class: deviled_eggs, Images: 250\n",
            "Class: donuts, Images: 250\n",
            "Class: dumplings, Images: 250\n",
            "Class: edamame, Images: 250\n",
            "Class: eggs_benedict, Images: 250\n",
            "Class: escargots, Images: 250\n",
            "Class: falafel, Images: 250\n",
            "Class: filet_mignon, Images: 250\n",
            "Class: fish_and_chips, Images: 250\n",
            "Class: foie_gras, Images: 250\n",
            "Class: french_fries, Images: 250\n",
            "Class: french_onion_soup, Images: 250\n",
            "Class: french_toast, Images: 250\n",
            "Class: fried_calamari, Images: 250\n",
            "Class: fried_rice, Images: 250\n",
            "Class: frozen_yogurt, Images: 250\n",
            "Class: garlic_bread, Images: 250\n",
            "Class: gnocchi, Images: 250\n",
            "Class: greek_salad, Images: 250\n",
            "Class: grilled_cheese_sandwich, Images: 250\n",
            "Class: grilled_salmon, Images: 250\n",
            "Class: guacamole, Images: 250\n",
            "Class: gyoza, Images: 250\n",
            "Class: hamburger, Images: 250\n",
            "Class: hot_and_sour_soup, Images: 250\n",
            "Class: hot_dog, Images: 250\n",
            "Class: huevos_rancheros, Images: 250\n",
            "Class: hummus, Images: 250\n",
            "Class: ice_cream, Images: 250\n",
            "Class: lasagna, Images: 250\n",
            "Class: lobster_bisque, Images: 250\n",
            "Class: lobster_roll_sandwich, Images: 250\n",
            "Class: macaroni_and_cheese, Images: 250\n",
            "Class: macarons, Images: 250\n",
            "Class: miso_soup, Images: 250\n",
            "Class: mussels, Images: 250\n",
            "Class: nachos, Images: 250\n",
            "Class: omelette, Images: 250\n",
            "Class: onion_rings, Images: 250\n",
            "Class: oysters, Images: 250\n",
            "Class: pad_thai, Images: 250\n",
            "Class: paella, Images: 250\n",
            "Class: pancakes, Images: 250\n",
            "Class: panna_cotta, Images: 250\n",
            "Class: peking_duck, Images: 250\n",
            "Class: pho, Images: 250\n",
            "Class: pizza, Images: 250\n",
            "Class: pork_chop, Images: 250\n",
            "Class: poutine, Images: 250\n",
            "Class: prime_rib, Images: 250\n",
            "Class: pulled_pork_sandwich, Images: 250\n",
            "Class: ramen, Images: 250\n",
            "Class: ravioli, Images: 250\n",
            "Class: red_velvet_cake, Images: 250\n",
            "Class: risotto, Images: 250\n",
            "Class: samosa, Images: 250\n",
            "Class: sashimi, Images: 250\n",
            "Class: scallops, Images: 250\n",
            "Class: seaweed_salad, Images: 250\n",
            "Class: shrimp_and_grits, Images: 250\n",
            "Class: spaghetti_bolognese, Images: 250\n",
            "Class: spaghetti_carbonara, Images: 250\n",
            "Class: spring_rolls, Images: 250\n",
            "Class: steak, Images: 250\n",
            "Class: strawberry_shortcake, Images: 250\n",
            "Class: sushi, Images: 250\n",
            "Class: tacos, Images: 250\n",
            "Class: takoyaki, Images: 250\n",
            "Class: tiramisu, Images: 250\n",
            "Class: tuna_tartare, Images: 250\n",
            "Class: waffles, Images: 250\n"
          ]
        }
      ],
      "source": [
        "def check_distribution(dataset_folder):\n",
        "    for root, dirs, files in os.walk(dataset_folder):\n",
        "        if len(files) > 0:\n",
        "            print(f\"Class: {os.path.basename(root)}, Images: {len(files)}\")\n",
        "\n",
        "print(\"Train Dataset Distribution:\")\n",
        "check_distribution(train_base_path)\n",
        "\n",
        "print(\"\\nTest Dataset Distribution:\")\n",
        "check_distribution(test_base_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================================================================================================\n",
            "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
            "============================================================================================================================================\n",
            "EfficientNet (EfficientNet)                                  [32, 3, 600, 600]    [32, 3]              --                   Partial\n",
            "├─Sequential (features)                                      [32, 3, 600, 600]    [32, 2560, 19, 19]   --                   False\n",
            "│    └─Conv2dNormActivation (0)                              [32, 3, 600, 600]    [32, 64, 300, 300]   --                   False\n",
            "│    │    └─Conv2d (0)                                       [32, 3, 600, 600]    [32, 64, 300, 300]   (1,728)              False\n",
            "│    │    └─BatchNorm2d (1)                                  [32, 64, 300, 300]   [32, 64, 300, 300]   (128)                False\n",
            "│    │    └─SiLU (2)                                         [32, 64, 300, 300]   [32, 64, 300, 300]   --                   --\n",
            "│    └─Sequential (1)                                        [32, 64, 300, 300]   [32, 32, 300, 300]   --                   False\n",
            "│    │    └─MBConv (0)                                       [32, 64, 300, 300]   [32, 32, 300, 300]   (4,944)              False\n",
            "│    │    └─MBConv (1)                                       [32, 32, 300, 300]   [32, 32, 300, 300]   (1,992)              False\n",
            "│    │    └─MBConv (2)                                       [32, 32, 300, 300]   [32, 32, 300, 300]   (1,992)              False\n",
            "│    │    └─MBConv (3)                                       [32, 32, 300, 300]   [32, 32, 300, 300]   (1,992)              False\n",
            "│    └─Sequential (2)                                        [32, 32, 300, 300]   [32, 48, 150, 150]   --                   False\n",
            "│    │    └─MBConv (0)                                       [32, 32, 300, 300]   [32, 48, 150, 150]   (21,224)             False\n",
            "│    │    └─MBConv (1)                                       [32, 48, 150, 150]   [32, 48, 150, 150]   (38,700)             False\n",
            "│    │    └─MBConv (2)                                       [32, 48, 150, 150]   [32, 48, 150, 150]   (38,700)             False\n",
            "│    │    └─MBConv (3)                                       [32, 48, 150, 150]   [32, 48, 150, 150]   (38,700)             False\n",
            "│    │    └─MBConv (4)                                       [32, 48, 150, 150]   [32, 48, 150, 150]   (38,700)             False\n",
            "│    │    └─MBConv (5)                                       [32, 48, 150, 150]   [32, 48, 150, 150]   (38,700)             False\n",
            "│    │    └─MBConv (6)                                       [32, 48, 150, 150]   [32, 48, 150, 150]   (38,700)             False\n",
            "│    └─Sequential (3)                                        [32, 48, 150, 150]   [32, 80, 75, 75]     --                   False\n",
            "│    │    └─MBConv (0)                                       [32, 48, 150, 150]   [32, 80, 75, 75]     (52,588)             False\n",
            "│    │    └─MBConv (1)                                       [32, 80, 75, 75]     [32, 80, 75, 75]     (110,580)            False\n",
            "│    │    └─MBConv (2)                                       [32, 80, 75, 75]     [32, 80, 75, 75]     (110,580)            False\n",
            "│    │    └─MBConv (3)                                       [32, 80, 75, 75]     [32, 80, 75, 75]     (110,580)            False\n",
            "│    │    └─MBConv (4)                                       [32, 80, 75, 75]     [32, 80, 75, 75]     (110,580)            False\n",
            "│    │    └─MBConv (5)                                       [32, 80, 75, 75]     [32, 80, 75, 75]     (110,580)            False\n",
            "│    │    └─MBConv (6)                                       [32, 80, 75, 75]     [32, 80, 75, 75]     (110,580)            False\n",
            "│    └─Sequential (4)                                        [32, 80, 75, 75]     [32, 160, 38, 38]    --                   False\n",
            "│    │    └─MBConv (0)                                       [32, 80, 75, 75]     [32, 160, 38, 38]    (141,460)            False\n",
            "│    │    └─MBConv (1)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    (397,800)            False\n",
            "│    │    └─MBConv (2)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    (397,800)            False\n",
            "│    │    └─MBConv (3)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    (397,800)            False\n",
            "│    │    └─MBConv (4)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    (397,800)            False\n",
            "│    │    └─MBConv (5)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    (397,800)            False\n",
            "│    │    └─MBConv (6)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    (397,800)            False\n",
            "│    │    └─MBConv (7)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    (397,800)            False\n",
            "│    │    └─MBConv (8)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    (397,800)            False\n",
            "│    │    └─MBConv (9)                                       [32, 160, 38, 38]    [32, 160, 38, 38]    (397,800)            False\n",
            "│    └─Sequential (5)                                        [32, 160, 38, 38]    [32, 224, 38, 38]    --                   False\n",
            "│    │    └─MBConv (0)                                       [32, 160, 38, 38]    [32, 224, 38, 38]    (474,728)            False\n",
            "│    │    └─MBConv (1)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    (793,464)            False\n",
            "│    │    └─MBConv (2)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    (793,464)            False\n",
            "│    │    └─MBConv (3)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    (793,464)            False\n",
            "│    │    └─MBConv (4)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    (793,464)            False\n",
            "│    │    └─MBConv (5)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    (793,464)            False\n",
            "│    │    └─MBConv (6)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    (793,464)            False\n",
            "│    │    └─MBConv (7)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    (793,464)            False\n",
            "│    │    └─MBConv (8)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    (793,464)            False\n",
            "│    │    └─MBConv (9)                                       [32, 224, 38, 38]    [32, 224, 38, 38]    (793,464)            False\n",
            "│    └─Sequential (6)                                        [32, 224, 38, 38]    [32, 384, 19, 19]    --                   False\n",
            "│    │    └─MBConv (0)                                       [32, 224, 38, 38]    [32, 384, 19, 19]    (1,008,824)          False\n",
            "│    │    └─MBConv (1)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
            "│    │    └─MBConv (2)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
            "│    │    └─MBConv (3)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
            "│    │    └─MBConv (4)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
            "│    │    └─MBConv (5)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
            "│    │    └─MBConv (6)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
            "│    │    └─MBConv (7)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
            "│    │    └─MBConv (8)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
            "│    │    └─MBConv (9)                                       [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
            "│    │    └─MBConv (10)                                      [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
            "│    │    └─MBConv (11)                                      [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
            "│    │    └─MBConv (12)                                      [32, 384, 19, 19]    [32, 384, 19, 19]    (2,281,824)          False\n",
            "│    └─Sequential (7)                                        [32, 384, 19, 19]    [32, 640, 19, 19]    --                   False\n",
            "│    │    └─MBConv (0)                                       [32, 384, 19, 19]    [32, 640, 19, 19]    (2,835,296)          False\n",
            "│    │    └─MBConv (1)                                       [32, 640, 19, 19]    [32, 640, 19, 19]    (6,199,200)          False\n",
            "│    │    └─MBConv (2)                                       [32, 640, 19, 19]    [32, 640, 19, 19]    (6,199,200)          False\n",
            "│    │    └─MBConv (3)                                       [32, 640, 19, 19]    [32, 640, 19, 19]    (6,199,200)          False\n",
            "│    └─Conv2dNormActivation (8)                              [32, 640, 19, 19]    [32, 2560, 19, 19]   --                   False\n",
            "│    │    └─Conv2d (0)                                       [32, 640, 19, 19]    [32, 2560, 19, 19]   (1,638,400)          False\n",
            "│    │    └─BatchNorm2d (1)                                  [32, 2560, 19, 19]   [32, 2560, 19, 19]   (5,120)              False\n",
            "│    │    └─SiLU (2)                                         [32, 2560, 19, 19]   [32, 2560, 19, 19]   --                   --\n",
            "├─AdaptiveAvgPool2d (avgpool)                                [32, 2560, 19, 19]   [32, 2560, 1, 1]     --                   --\n",
            "├─Sequential (classifier)                                    [32, 2560]           [32, 3]              --                   True\n",
            "│    └─Dropout (0)                                           [32, 2560]           [32, 2560]           --                   --\n",
            "│    └─Linear (1)                                            [32, 2560]           [32, 3]              7,683                True\n",
            "============================================================================================================================================\n",
            "Total params: 63,794,643\n",
            "Trainable params: 7,683\n",
            "Non-trainable params: 63,786,960\n",
            "Total mult-adds (Units.TERABYTES): 1.21\n",
            "============================================================================================================================================\n",
            "Input size (MB): 138.24\n",
            "Forward/backward pass size (MB): 148429.76\n",
            "Params size (MB): 255.18\n",
            "Estimated Total Size (MB): 148823.17\n",
            "============================================================================================================================================\n",
            " number of input_features of final layter 2560\n"
          ]
        }
      ],
      "source": [
        "# TODO: your code\n",
        "# effnetb7_weights = torchvision.models.EfficientNet_B7_Weights.DEFAULT\n",
        "# effnetb7 = torchvision.models.efficientnet_b7(weights=effnetb7_weights)\n",
        "\n",
        "print (summary (model= effnetb7,\n",
        "       input_size = (32, 3, 600, 600),\n",
        "       col_names = ['input_size', 'output_size', 'num_params', 'trainable'],\n",
        "       col_width = 20,\n",
        "       row_settings = ['var_names']))\n",
        "\n",
        "print (f' number of input_features of final layter {len (effnetb7.classifier.state_dict()[\"1.weight\"][0])}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup Directory and Metadata Paths\n",
        "First, ensure that you have the path to the images directory and metadata files (train.txt and test.txt)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import os\n",
        "# from torchvision import datasets, transforms\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# from PIL import Image\n",
        "\n",
        "# # Paths\n",
        "# images_dir = '../data/food-101/images'  # Path to the images directory\n",
        "# train_metadata_path = '../data/food-101/meta/train.txt'\n",
        "# test_metadata_path = '../data/food-101/meta/test.txt'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create a Custom Dataset Class\n",
        "Create a custom dataset class that reads the image paths from the metadata files and loads the images with corresponding labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# class Food101Dataset(Dataset):\n",
        "#     def __init__(self, image_dir, metadata_file, transform=None):\n",
        "#         self.image_dir = image_dir\n",
        "#         self.transform = transform\n",
        "#         self.image_labels = []\n",
        "        \n",
        "#         # Read the metadata file and store image paths and labels\n",
        "#         with open(metadata_file, 'r') as file:\n",
        "#             lines = file.readlines()\n",
        "#             for line in lines:\n",
        "#                 image_path = line.strip() + '.jpg'\n",
        "#                 label = line.strip().split('/')[0]  # The class label is the folder name\n",
        "#                 self.image_labels.append((image_path, label))\n",
        "        \n",
        "#         # Create a mapping from class labels to numerical labels\n",
        "#         self.classes = sorted(set(label for _, label in self.image_labels))\n",
        "#         self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
        "        \n",
        "#     def __len__(self):\n",
        "#         return len(self.image_labels)\n",
        "    \n",
        "#     def __getitem__(self, idx):\n",
        "#         image_path, label = self.image_labels[idx]\n",
        "#         image = Image.open(os.path.join(self.image_dir, image_path)).convert(\"RGB\")\n",
        "#         label_idx = self.class_to_idx[label]\n",
        "        \n",
        "#         if self.transform:\n",
        "#             image = self.transform(image)\n",
        "        \n",
        "#         return image, label_idx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define Data Transformations\n",
        "Define the necessary data transformations for training and testing. You can use different transformations for each phase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Define transformations\n",
        "# train_transforms = transforms.Compose([\n",
        "#     transforms.Resize((224, 224)),\n",
        "#     transforms.RandomHorizontalFlip(),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "# ])\n",
        "\n",
        "# test_transforms = transforms.Compose([\n",
        "#     transforms.Resize((224, 224)),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "# ])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### . Create Dataset Instances\n",
        "Create instances of the Food101Dataset class for both training and testing datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Create dataset instances\n",
        "# train_dataset = Food101Dataset(image_dir=images_dir, metadata_file=train_metadata_path, transform=train_transforms)\n",
        "# test_dataset = Food101Dataset(image_dir=images_dir, metadata_file=test_metadata_path, transform=test_transforms)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  Create DataLoaders\n",
        "Create DataLoaders for both the training and testing datasets. The DataLoader will handle batching, shuffling, and other operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # Create DataLoaders\n",
        "# batch_size = 32\n",
        "\n",
        "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verify the DataLoader\n",
        "You can verify that the DataLoader is working correctly by iterating through it and visualizing some images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Function to display a batch of images\n",
        "# def show_images(images, labels, class_names):\n",
        "#     fig, axes = plt.subplots(1, len(images), figsize=(12, 12))\n",
        "#     for img, label, ax in zip(images, labels, axes):\n",
        "#         ax.imshow(img.permute(1, 2, 0).numpy())\n",
        "#         ax.set_title(class_names[label])\n",
        "#         ax.axis('off')\n",
        "#     plt.show()\n",
        "\n",
        "# # Get a batch of training data\n",
        "# images, labels = next(iter(train_loader))\n",
        "\n",
        "# # Show a batch of images\n",
        "# show_images(images[:5], labels[:5], train_dataset.classes)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyP4+Nwb43yrG43qNz11d5C4",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "07_pytorch_experiment_tracking_exercise_template.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
