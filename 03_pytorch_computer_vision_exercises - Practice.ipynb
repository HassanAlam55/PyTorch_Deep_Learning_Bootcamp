{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/03_pytorch_computer_vision_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vex99np2wFVt"
      },
      "source": [
        "# 03. PyTorch Computer Vision Exercises\n",
        "\n",
        "The following is a collection of exercises based on computer vision fundamentals in PyTorch.\n",
        "\n",
        "They're a bunch of fun.\n",
        "\n",
        "You're going to get to write plenty of code!\n",
        "\n",
        "## Resources\n",
        "\n",
        "1. These exercises are based on [notebook 03 of the Learn PyTorch for Deep Learning course](https://www.learnpytorch.io/03_pytorch_computer_vision/). \n",
        "2. See a live [walkthrough of the solutions (errors and all) on YouTube](https://youtu.be/_PibmqpEyhA). \n",
        "  * **Note:** Going through these exercises took me just over 3 hours of solid coding, so you should expect around the same.\n",
        "3. See [other solutions on the course GitHub](https://github.com/mrdbourke/pytorch-deep-learning/tree/main/extras/solutions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaeYzOTLwWh2",
        "outputId": "17dd5453-9639-4b01-aa18-7ddbfd5c3253"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Aug  7 12:35:10 2024       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 551.86                 Driver Version: 551.86         CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                     TCC/WDDM  | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA GeForce RTX 3080      WDDM  |   00000000:01:00.0  On |                  N/A |\n",
            "| 53%   33C    P8             56W /  370W |    8161MiB /  10240MiB |     33%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|    0   N/A  N/A      1612    C+G   ...n\\126.0.2592.113\\msedgewebview2.exe      N/A      |\n",
            "|    0   N/A  N/A      2900    C+G   ...m Files\\Mozilla Firefox\\firefox.exe      N/A      |\n",
            "|    0   N/A  N/A      3404    C+G   ..._8wekyb3d8bbwe\\Microsoft.Photos.exe      N/A      |\n",
            "|    0   N/A  N/A      4148    C+G   ...Mozilla Thunderbird\\thunderbird.exe      N/A      |\n",
            "|    0   N/A  N/A      5932    C+G   ...oogle\\Chrome\\Application\\chrome.exe      N/A      |\n",
            "|    0   N/A  N/A      7608    C+G   ...n\\126.0.2592.113\\msedgewebview2.exe      N/A      |\n",
            "|    0   N/A  N/A      7848    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe      N/A      |\n",
            "|    0   N/A  N/A      7980    C+G   C:\\Windows\\explorer.exe                     N/A      |\n",
            "|    0   N/A  N/A      9548    C+G   ...Programs\\Microsoft VS Code\\Code.exe      N/A      |\n",
            "|    0   N/A  N/A     10140    C+G   ....Search_cw5n1h2txyewy\\SearchApp.exe      N/A      |\n",
            "|    0   N/A  N/A     10604    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe      N/A      |\n",
            "|    0   N/A  N/A     11308    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe      N/A      |\n",
            "|    0   N/A  N/A     12748    C+G   ...Wacom\\WacomCenter\\WacomCenterUI.exe      N/A      |\n",
            "|    0   N/A  N/A     12888    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe      N/A      |\n",
            "|    0   N/A  N/A     15328    C+G   ...crosoft\\Skype for Desktop\\Skype.exe      N/A      |\n",
            "|    0   N/A  N/A     15636    C+G   ...es (x86)\\Dropbox\\Client\\Dropbox.exe      N/A      |\n",
            "|    0   N/A  N/A     16148    C+G   ...0.0_x64__cv1g1gvanyjgm\\WhatsApp.exe      N/A      |\n",
            "|    0   N/A  N/A     16304    C+G   ...l\\Microsoft\\Teams\\current\\Teams.exe      N/A      |\n",
            "|    0   N/A  N/A     16564    C+G   ...a\\Local\\slack\\app-4.39.95\\slack.exe      N/A      |\n",
            "|    0   N/A  N/A     17208    C+G   ...al\\Discord\\app-1.0.9156\\Discord.exe      N/A      |\n",
            "|    0   N/A  N/A     18508    C+G   ...es (x86)\\Dropbox\\Client\\Dropbox.exe      N/A      |\n",
            "|    0   N/A  N/A     18960    C+G   ...l\\Microsoft\\Teams\\current\\Teams.exe      N/A      |\n",
            "|    0   N/A  N/A     19152    C+G   ..._x64__kzf8qxf38zg5c\\Skype\\Skype.exe      N/A      |\n",
            "|    0   N/A  N/A     20188    C+G   ...ft Office\\root\\Office16\\WINWORD.EXE      N/A      |\n",
            "|    0   N/A  N/A     24732    C+G   ...m Files\\Mozilla Firefox\\firefox.exe      N/A      |\n",
            "|    0   N/A  N/A     25268    C+G   ...siveControlPanel\\SystemSettings.exe      N/A      |\n",
            "|    0   N/A  N/A     27784    C+G   ...soft Office\\root\\Office16\\EXCEL.EXE      N/A      |\n",
            "|    0   N/A  N/A     32744      C   ...naconda3\\envs\\py311Udemy\\python.exe      N/A      |\n",
            "|    0   N/A  N/A     41052    C+G   ...r\\AppData\\Roaming\\Zoom\\bin\\Zoom.exe      N/A      |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check for GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Tiger\\anaconda3\\envs\\py311Udemy\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_circles\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import make_blobs\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from helper_functions import plot_predictions, plot_decision_boundary, accuracy_fn\n",
        "from torchmetrics import Accuracy, ConfusionMatrix\n",
        "import mlxtend\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "from timeit import default_timer as timer\n",
        "from tqdm.auto import tqdm\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "import random\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "DNwZLMbCzJLk",
        "outputId": "9c150c50-a092-4f34-9d33-b45247fb080d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.3.0\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Import torch\n",
        "import torch\n",
        "\n",
        "# Exercises require PyTorch > 1.10.0\n",
        "print(torch.__version__)\n",
        "\n",
        "# TODO: Setup device agnostic code\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print (device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSFX7tc1w-en"
      },
      "source": [
        "## 1. What are 3 areas in industry where computer vision is currently being used?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyWRkvWGbCXj"
      },
      "source": [
        "1. Image recognition\n",
        "2. Self Driving\n",
        "3. Medical imaging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBK-WI6YxDYa"
      },
      "source": [
        "## 2. Search \"what is overfitting in machine learning\" and write down a sentence about what you find. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1rxD6GObCqh"
      },
      "source": [
        "fits training data well, but not genaralizable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeYFEqw8xK26"
      },
      "source": [
        "## 3. Search \"ways to prevent overfitting in machine learning\", write down 3 of the things you find and a sentence about each. \n",
        "> **Note:** there are lots of these, so don't worry too much about all of them, just pick 3 and start with those."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocvOdWKcbEKr"
      },
      "source": [
        "1. dropout\n",
        "2. stop training when test results are worse than training\n",
        "3. introduce noixe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKdEEFEqxM-8"
      },
      "source": [
        "## 4. Spend 20-minutes reading and clicking through the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/).\n",
        "\n",
        "* Upload your own example image using the \"upload\" button on the website and see what happens in each layer of a CNN as your image passes through it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqZaJIRMbFtS"
      },
      "source": [
        "classifies graph as lady bug"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvf-3pODxXYI"
      },
      "source": [
        "## 5. Load the [`torchvision.datasets.MNIST()`](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html#torchvision.datasets.MNIST) train and test datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SHjeuN81bHza"
      },
      "outputs": [],
      "source": [
        "train_data = datasets.MNIST(\n",
        "    root = 'data',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = ToTensor(),\n",
        "    target_transform = None\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root = 'data',\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = ToTensor(),\n",
        "    target_transform = None\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxZW-uAbxe_F"
      },
      "source": [
        "## 6. Visualize at least 5 different samples of the MNIST training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QVFsYi1PbItE"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 900x900 with 6 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "train_data_size = len (train_data)\n",
        "fig = plt.figure (figsize=(9,9))\n",
        "image_count_range = range (1, 7)\n",
        "rows, cols = 3, 2\n",
        "for i in  image_count_range:\n",
        "    img_idx = random.randint(0, train_data_size)\n",
        "    image, label = train_data[img_idx]\n",
        "    fig.add_subplot (rows, cols, i)\n",
        "    plt.imshow (image.squeeze(), cmap = 'grey')\n",
        "    plt.title (label)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset MNIST\n",
              "    Number of datapoints: 60000\n",
              "    Root location: data\n",
              "    Split: Train\n",
              "    StandardTransform\n",
              "Transform: ToTensor()"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAPDzW0wxhi3"
      },
      "source": [
        "## 7. Turn the MNIST train and test datasets into dataloaders using `torch.utils.data.DataLoader`, set the `batch_size=32`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ALA6MPcFbJXQ"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "train_dataloader = DataLoader(dataset=train_data,\n",
        "                            batch_size = BATCH_SIZE,\n",
        "                            shuffle = True)\n",
        "\n",
        "test_dataloader = DataLoader(dataset = test_data,\n",
        "                            batch_size = BATCH_SIZE,\n",
        "                            shuffle = False\n",
        "                            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCCVfXk5xjYS"
      },
      "source": [
        "## 8. Recreate `model_2` used in notebook 03 (the same model from the [CNN Explainer website](https://poloclub.github.io/cnn-explainer/), also known as TinyVGG) capable of fitting on the MNIST dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image shape torch.Size([1, 28, 28])\n",
            "lazy out torch.Size([1, 10])\n",
            "image shape torch.Size([1, 28, 28])\n",
            "max_pool shape torch.Size([1, 10, 13, 13]),\n",
            "flatten_out torch.Size([1, 1690])\n"
          ]
        }
      ],
      "source": [
        "# write test code to compute size of \n",
        "torch.manual_seed(42)\n",
        "img, img_class = train_data[0]\n",
        "print (f'image shape {img.shape}')\n",
        "conv_layer = nn.Conv2d(in_channels = 1, \n",
        "                out_channels = 10,\n",
        "                kernel_size = 3,\n",
        "                padding = 0)\n",
        "max_pool_layer = nn.MaxPool2d(kernel_size = 2)\n",
        "flatten_model = nn.Flatten()\n",
        "lazy_dense = nn.LazyLinear(out_features=10)\n",
        "# linear_model = nn.Linear(in_features=13*13,\n",
        "#                         out_features = 10)\n",
        "\n",
        "unsqueeze_img = img.unsqueeze (dim = 0)\n",
        "conv_out = conv_layer(unsqueeze_img)\n",
        "# print( conv_out.view(1, -1).size(1))\n",
        "maxp_out = max_pool_layer(conv_out)\n",
        "# print( maxp_out.view(1, -1).size(1))\n",
        "flat_out = flatten_model(maxp_out)\n",
        "# linear_out = linear_model(flat_out)\n",
        "flat_out.shape, maxp_out.shape, linear_out.shape\n",
        "final_out = lazy_dense(flat_out)\n",
        "flat_out.shape[0]*flat_out.shape[1]\n",
        "# final_out.shape, flat_out.shape, maxp_out.shape, linear_out.shape\n",
        "print (f'lazy out {final_out.shape}\\nimage shape {img.shape}\\nmax_pool shape {maxp_out.shape},\\nflatten_out {flat_out.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "unqueeze img torch.Size([1, 1, 28, 28])\n",
            "torch.Size([1, 10])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Tiger\\anaconda3\\envs\\py311Udemy\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the network architecture\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.conv_layer = nn.Conv2d(in_channels=1, out_channels=10, kernel_size=3, padding=0)\n",
        "        self.max_pool_layer = nn.MaxPool2d(kernel_size=2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_layer = nn.Linear(in_features=10*13*13, out_features=10)\n",
        "        self.lazy_layer = nn.LazyLinear(out_features=10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.conv_layer(x)\n",
        "        x = self.max_pool_layer(x)\n",
        "        x = self.flatten(x)\n",
        "        # x = self.linear_layer(x)\n",
        "        x = self.lazy_layer(x)\n",
        "        return x\n",
        "\n",
        "# Create a dummy input tensor with appropriate dimensions (1 channel, 14x14)\n",
        "dummy_input = torch.randn(1, 1, 14, 14)\n",
        "# print (f'input_shape {dummy_input.shape}')\n",
        "# Instantiate the model and get the output\n",
        "model = CustomModel()\n",
        "# output = model(dummy_input)\n",
        "unsq_img = img.unsqueeze(dim=0)\n",
        "print (f'unqueeze img {unsq_img.shape}')\n",
        "output = model(unsq_img)\n",
        "print(output.shape)  # should be torch.Size([1, 10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "5IKNF22XbKYS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Tiger\\anaconda3\\envs\\py311Udemy\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "HassanCNN(\n",
              "  (conv1): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (relu): ReLU()\n",
              "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "  (lazydense): LazyLinear(in_features=0, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class HassanCNN(nn.Module):\n",
        "    def __init__ (self,input_shape, hidden_units, output_shape,\n",
        "            in_conv_k_size:int = 3,\n",
        "            in_stride:int = 1,\n",
        "            in_pad:int = 1,\n",
        "            in_maxp_k_size = 2,\n",
        "            in_maxp_stride = 2,\n",
        "            in_batch_size = 32):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "                            in_channels = input_shape,\n",
        "                            out_channels = hidden_units,\n",
        "                            kernel_size=in_conv_k_size,\n",
        "                            stride = in_stride,\n",
        "                            padding = in_pad)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "                            in_channels = hidden_units,\n",
        "                            out_channels = hidden_units,\n",
        "                            kernel_size=in_conv_k_size,\n",
        "                            stride = in_stride,\n",
        "                            padding = in_pad)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=in_maxp_k_size,\n",
        "                                    stride = in_maxp_stride)\n",
        "        self.flatten = nn.Flatten()\n",
        "        # self.linear = nn.Linear()\n",
        "        self.lazydense = nn.LazyLinear(out_features = output_shape)\n",
        "\n",
        "    def forward (self, x):\n",
        "        # first block\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        \n",
        "\n",
        "        # second block\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        # flatten\n",
        "        # linear_output = x.size(1) * x.size(2) * hidden_units\n",
        "        x = self.flatten(x)\n",
        "        # flattened_size = x.view(in_batch_size, -1).size(1)\n",
        "        # flattened_features = hidden_units * flattened_size\n",
        "        # flattened_feature = x.shape[0] * x.shape[1]\n",
        "        x = self.lazydense (x)\n",
        "        return x\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model_has1 = HassanCNN(input_shape = 1,\n",
        "                       hidden_units = 10,\n",
        "                       output_shape = 10)\n",
        "\n",
        "model_has1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 10])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X, y = train_data[0]\n",
        "pred = model_has1(X.unsqueeze(dim =0))\n",
        "pred.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_3zUr7xlhy"
      },
      "source": [
        "## 9. Train the model you built in exercise 8. for 5 epochs on CPU and GPU and see how long it takes on each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "jSo6vVWFbNLD"
      },
      "outputs": [],
      "source": [
        "# set up loss\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params = model_has1.parameters(), lr = 0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "# timer\n",
        "def print_train_time (start:float,\n",
        "                      end:float,\n",
        "                      device:torch.device = None):\n",
        "    total_time = end - start\n",
        "    print (f'Train time ondevice {device}: {total_time:.3f} seconds')\n",
        "    return total_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_loop (model: nn.Module,\n",
        "                dataloader:torch.utils.data.DataLoader,\n",
        "                loss_fn:torch.nn.Module,\n",
        "                optimizer:torch.optim.Optimizer,\n",
        "                accuracy_fn,\n",
        "                device:torch.device = device):\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        y_train_pred = model (X)\n",
        "        loss = loss_fn (y_train_pred, y)\n",
        "        train_loss += loss\n",
        "        train_acc += accuracy_fn(y_true = y,\n",
        "                                 y_pred = y_train_pred.argmax(dim = 1))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    train_loss /= len(dataloader)\n",
        "    train_acc /= len(dataloader)\n",
        "    print (f'Train Loss {train_loss:.4f}| train acc {train_acc:.2f} ')\n",
        "\n",
        "def test_loop (mode: nn.Module,\n",
        "               dataloader:torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               accuracy_fn,\n",
        "               device:torch.device = device):\n",
        "    \n",
        "    test_loss = 0\n",
        "    test_acc = 0\n",
        "    model.eval()\n",
        "    for batch, (X, y) in enumerate (dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        y_test_pred = model(X)\n",
        "        test_loss += loss_fn(y_test_pred, y)\n",
        "        test_acc += accuracy_fn(y_true = y, \n",
        "                                y_pred = y_test_pred.argmax(dim = 1))\n",
        "        \n",
        "    test_loss /= len(dataloader)\n",
        "    test_acc /= len(dataloader)\n",
        "\n",
        "    print ('Test Loss: {test_loss:.4f}| Test ACC {test_acc:.2f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0\n",
            "\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[42], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm (range_epochs):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m     train_loop(model\u001b[38;5;241m=\u001b[39m model_has1,\n\u001b[0;32m      8\u001b[0m            dataloader \u001b[38;5;241m=\u001b[39m train_dataloader,\n\u001b[0;32m      9\u001b[0m            loss_fn \u001b[38;5;241m=\u001b[39m loss_fn,\n\u001b[0;32m     10\u001b[0m            optimizer \u001b[38;5;241m=\u001b[39m optimizer,\n\u001b[0;32m     11\u001b[0m            accuracy_fn \u001b[38;5;241m=\u001b[39maccuracy_fn)\n\u001b[0;32m     13\u001b[0m     test_loop(model\u001b[38;5;241m=\u001b[39m model_has1,\n\u001b[0;32m     14\u001b[0m               data_loader \u001b[38;5;241m=\u001b[39m test_dataloader,\n\u001b[0;32m     15\u001b[0m               loss_fn \u001b[38;5;241m=\u001b[39m loss_fn,\n\u001b[0;32m     16\u001b[0m               accuracy_fn \u001b[38;5;241m=\u001b[39m accuracy_fn)\n\u001b[0;32m     19\u001b[0m train_time_end_on_has_model1 \u001b[38;5;241m=\u001b[39m timer()\n",
            "Cell \u001b[1;32mIn[41], line 12\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(model, dataloader, loss_fn, optimizer, accuracy_fn, device)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch, (X, y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m     11\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 12\u001b[0m     y_train_pred \u001b[38;5;241m=\u001b[39m model (X)\n\u001b[0;32m     13\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn (y_train_pred, y)\n\u001b[0;32m     14\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
            "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\py311Udemy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\py311Udemy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[1;32mIn[30], line 32\u001b[0m, in \u001b[0;36mHassanCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m (\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# first block\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[0;32m     33\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m     34\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool(x)\n",
            "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\py311Udemy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\py311Udemy\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\py311Udemy\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
            "File \u001b[1;32mc:\\Users\\Tiger\\anaconda3\\envs\\py311Udemy\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "train_time_start_on_has_model1 = timer()\n",
        "epochs = 5\n",
        "range_epochs = range (epochs)\n",
        "for epoch in tqdm (range_epochs):\n",
        "    print (f'Epoch {epoch}\\n')\n",
        "    train_loop(model= model_has1,\n",
        "           dataloader = train_dataloader,\n",
        "           loss_fn = loss_fn,\n",
        "           optimizer = optimizer,\n",
        "           accuracy_fn =accuracy_fn)\n",
        "    \n",
        "    test_loop(model= model_has1,\n",
        "              data_loader = test_dataloader,\n",
        "              loss_fn = loss_fn,\n",
        "              accuracy_fn = accuracy_fn)\n",
        "    \n",
        "\n",
        "train_time_end_on_has_model1 = timer()\n",
        "\n",
        "total_time_has_model1 =print_train_time(start = train_time_start_on_has_model1,\n",
        "                                        end=train_time_end_on_has_model1,\n",
        "                                        device = device)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1CsHhPpxp1w"
      },
      "source": [
        "## 10. Make predictions using your trained model and visualize at least 5 of them comparing the prediciton to the target label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YGgZvSobNxu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQwzqlBWxrpG"
      },
      "source": [
        "## 11. Plot a confusion matrix comparing your model's predictions to the truth labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSrXiT_AbQ6e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lj6bDhoWxt2y"
      },
      "source": [
        "## 12. Create a random tensor of shape `[1, 3, 64, 64]` and pass it through a `nn.Conv2d()` layer with various hyperparameter settings (these can be any settings you choose), what do you notice if the `kernel_size` parameter goes up and down?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "leCTsqtSbR5P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHS20cNTxwSi"
      },
      "source": [
        "## 13. Use a model similar to the trained `model_2` from notebook 03 to make predictions on the test [`torchvision.datasets.FashionMNIST`](https://pytorch.org/vision/main/generated/torchvision.datasets.FashionMNIST.html) dataset. \n",
        "* Then plot some predictions where the model was wrong alongside what the label of the image should've been. \n",
        "* After visualing these predictions do you think it's more of a modelling error or a data error? \n",
        "* As in, could the model do better or are the labels of the data too close to each other (e.g. a \"Shirt\" label is too close to \"T-shirt/top\")?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78a8LjtdbSZj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMUsDcN/+FAm9Pf7Ifqs6AZ",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "03_pytorch_computer_vision_exercises.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
