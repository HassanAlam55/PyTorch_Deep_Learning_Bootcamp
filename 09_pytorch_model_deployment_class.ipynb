{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment Class, replicating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import accuracy_fn, download_data, plot_decision_boundary, plot_loss_curves, plot_predictions, set_seeds\n",
    "# from helper_functions import download_data, set_seeds, plot_loss_curves\n",
    "from going_modular import engine, data_loaders\n",
    "from going_modular import predictions\n",
    "from going_modular.engine import train_step, test_step\n",
    "from going_modular.utils import save_model\n",
    "import mlxtend\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import random\n",
    "import requests\n",
    "import sklearn\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_blobs\n",
    "from torchinfo import summary\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchmetrics import Accuracy, ConfusionMatrix\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from timeit import default_timer as timer\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, Dict, List\n",
    "writer = SummaryWriter()\n",
    "import zipfile\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0+cpu\n",
      "0.20.0+cpu\n"
     ]
    }
   ],
   "source": [
    "# see torch and torch vision plus\n",
    "print (torch.__version__)\n",
    "print (torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "def set_seeds (seed: int=42):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): _description_. Defaults to 42.\n",
    "    \"\"\" \n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "effnetb2 transforms: ImageClassification(\n",
      "    crop_size=[288]\n",
      "    resize_size=[288]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BICUBIC\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# wet up data paths\n",
    "# E:\\Dropbox\\GithubRepo\\Udemy\\pytorch-deep-learning-main\\pytorch-deep-learning-main - Copy\\data\\pizza_steak_sushi_20_percent\n",
    "data_20_percent_path = Path('../data/pizza_steak_sushi_20_percent')\n",
    "train_dir = data_20_percent_path / 'train'\n",
    "test_dir = data_20_percent_path / 'test'\n",
    "effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "effnetb2_transforms = effnetb2_weights.transforms()\n",
    "print (f'effnetb2 transforms: {effnetb2_transforms}')\n",
    "# effnetb2 = torchvision.models.efficientnet_b2(weights=\"effnetb2_weights\")\n",
    "effnetb2 = torchvision.models.efficientnet_b2(weights=\"DEFAULT\")\n",
    "\n",
    "for param in effnetb2.parameters():\n",
    "    param.requires_grad=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "EfficientNet (EfficientNet)                                  [1, 3, 224, 224]     [1, 1000]            --                   False\n",
       "├─Sequential (features)                                      [1, 3, 224, 224]     [1, 1408, 7, 7]      --                   False\n",
       "│    └─Conv2dNormActivation (0)                              [1, 3, 224, 224]     [1, 32, 112, 112]    --                   False\n",
       "│    │    └─Conv2d (0)                                       [1, 3, 224, 224]     [1, 32, 112, 112]    (864)                False\n",
       "│    │    └─BatchNorm2d (1)                                  [1, 32, 112, 112]    [1, 32, 112, 112]    (64)                 False\n",
       "│    │    └─SiLU (2)                                         [1, 32, 112, 112]    [1, 32, 112, 112]    --                   --\n",
       "│    └─Sequential (1)                                        [1, 32, 112, 112]    [1, 16, 112, 112]    --                   False\n",
       "│    │    └─MBConv (0)                                       [1, 32, 112, 112]    [1, 16, 112, 112]    (1,448)              False\n",
       "│    │    └─MBConv (1)                                       [1, 16, 112, 112]    [1, 16, 112, 112]    (612)                False\n",
       "│    └─Sequential (2)                                        [1, 16, 112, 112]    [1, 24, 56, 56]      --                   False\n",
       "│    │    └─MBConv (0)                                       [1, 16, 112, 112]    [1, 24, 56, 56]      (6,004)              False\n",
       "│    │    └─MBConv (1)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      (10,710)             False\n",
       "│    │    └─MBConv (2)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      (10,710)             False\n",
       "│    └─Sequential (3)                                        [1, 24, 56, 56]      [1, 48, 28, 28]      --                   False\n",
       "│    │    └─MBConv (0)                                       [1, 24, 56, 56]      [1, 48, 28, 28]      (16,518)             False\n",
       "│    │    └─MBConv (1)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      (43,308)             False\n",
       "│    │    └─MBConv (2)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      (43,308)             False\n",
       "│    └─Sequential (4)                                        [1, 48, 28, 28]      [1, 88, 14, 14]      --                   False\n",
       "│    │    └─MBConv (0)                                       [1, 48, 28, 28]      [1, 88, 14, 14]      (50,300)             False\n",
       "│    │    └─MBConv (1)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
       "│    │    └─MBConv (2)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
       "│    │    └─MBConv (3)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
       "│    └─Sequential (5)                                        [1, 88, 14, 14]      [1, 120, 14, 14]     --                   False\n",
       "│    │    └─MBConv (0)                                       [1, 88, 14, 14]      [1, 120, 14, 14]     (149,158)            False\n",
       "│    │    └─MBConv (1)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
       "│    │    └─MBConv (2)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
       "│    │    └─MBConv (3)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
       "│    └─Sequential (6)                                        [1, 120, 14, 14]     [1, 208, 7, 7]       --                   False\n",
       "│    │    └─MBConv (0)                                       [1, 120, 14, 14]     [1, 208, 7, 7]       (301,406)            False\n",
       "│    │    └─MBConv (1)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
       "│    │    └─MBConv (2)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
       "│    │    └─MBConv (3)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
       "│    │    └─MBConv (4)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
       "│    └─Sequential (7)                                        [1, 208, 7, 7]       [1, 352, 7, 7]       --                   False\n",
       "│    │    └─MBConv (0)                                       [1, 208, 7, 7]       [1, 352, 7, 7]       (846,900)            False\n",
       "│    │    └─MBConv (1)                                       [1, 352, 7, 7]       [1, 352, 7, 7]       (1,888,920)          False\n",
       "│    └─Conv2dNormActivation (8)                              [1, 352, 7, 7]       [1, 1408, 7, 7]      --                   False\n",
       "│    │    └─Conv2d (0)                                       [1, 352, 7, 7]       [1, 1408, 7, 7]      (495,616)            False\n",
       "│    │    └─BatchNorm2d (1)                                  [1, 1408, 7, 7]      [1, 1408, 7, 7]      (2,816)              False\n",
       "│    │    └─SiLU (2)                                         [1, 1408, 7, 7]      [1, 1408, 7, 7]      --                   --\n",
       "├─AdaptiveAvgPool2d (avgpool)                                [1, 1408, 7, 7]      [1, 1408, 1, 1]      --                   --\n",
       "├─Sequential (classifier)                                    [1, 1408]            [1, 1000]            --                   False\n",
       "│    └─Dropout (0)                                           [1, 1408]            [1, 1408]            --                   --\n",
       "│    └─Linear (1)                                            [1, 1408]            [1, 1000]            (1,409,000)          False\n",
       "============================================================================================================================================\n",
       "Total params: 9,109,994\n",
       "Trainable params: 0\n",
       "Non-trainable params: 9,109,994\n",
       "Total mult-adds (Units.MEGABYTES): 659.05\n",
       "============================================================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 156.81\n",
       "Params size (MB): 36.44\n",
       "Estimated Total Size (MB): 193.85\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "# effnetb2\n",
    "summary(model=effnetb2,\n",
    "        input_size= (1, 3, 224, 224),\n",
    "        col_names=['input_size', 'output_size', 'num_params', 'trainable'],\n",
    "        col_width=20,\n",
    "        row_settings=['var_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1408"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get input size of last layer\n",
    "effnetb2_lastlayer = list(effnetb2.children())[-1]\n",
    "effnetb2_last_child_layer_inpu_count = list(effnetb2_lastlayer.children())[-1].in_features\n",
    "effnetb2_last_child_layer_inpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds()\n",
    "effnetb2.classifier=nn.Sequential(\n",
    "    nn.Dropout(p = 0.3, inplace=True),\n",
    "    nn.Linear(in_features=effnetb2_last_child_layer_inpu_count, out_features=3)\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create effnetb2\n",
    "def create_effnetb2(output_feature_count:int, drop_out = 0.3, seed:int=42)-> nn.Module:\n",
    "    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
    "    transforms= weights.transforms()\n",
    "    model = torchvision.models.efficientnet_b2(weights=\"DEFAULT\").to(device)\n",
    "\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    set_seeds()\n",
    "\n",
    "    last_layer = list(model.children())[-1]\n",
    "    last_child_layer = list(last_layer.children())[-1]\n",
    "    last_input = last_child_layer.in_features\n",
    "\n",
    "    model.classifer = nn.Sequential(\n",
    "        nn.Dropout(p = drop_out, inplace=True),\n",
    "        nn.Linear(in_features =last_input, \n",
    "                  out_features = output_feature_count).to(device))\n",
    "\n",
    "    model.name = 'effnetb2'\n",
    "    print (f'[INFO] created new {model.name} model')\n",
    "    \n",
    "    return model, transforms\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] created new effnetb2 model\n",
      "============================================================================================================================================\n",
      "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
      "============================================================================================================================================\n",
      "EfficientNet (EfficientNet)                                  [1, 3, 224, 224]     [1, 1000]            4,227                Partial\n",
      "├─Sequential (features)                                      [1, 3, 224, 224]     [1, 1408, 7, 7]      --                   False\n",
      "│    └─Conv2dNormActivation (0)                              [1, 3, 224, 224]     [1, 32, 112, 112]    --                   False\n",
      "│    │    └─Conv2d (0)                                       [1, 3, 224, 224]     [1, 32, 112, 112]    (864)                False\n",
      "│    │    └─BatchNorm2d (1)                                  [1, 32, 112, 112]    [1, 32, 112, 112]    (64)                 False\n",
      "│    │    └─SiLU (2)                                         [1, 32, 112, 112]    [1, 32, 112, 112]    --                   --\n",
      "│    └─Sequential (1)                                        [1, 32, 112, 112]    [1, 16, 112, 112]    --                   False\n",
      "│    │    └─MBConv (0)                                       [1, 32, 112, 112]    [1, 16, 112, 112]    (1,448)              False\n",
      "│    │    └─MBConv (1)                                       [1, 16, 112, 112]    [1, 16, 112, 112]    (612)                False\n",
      "│    └─Sequential (2)                                        [1, 16, 112, 112]    [1, 24, 56, 56]      --                   False\n",
      "│    │    └─MBConv (0)                                       [1, 16, 112, 112]    [1, 24, 56, 56]      (6,004)              False\n",
      "│    │    └─MBConv (1)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      (10,710)             False\n",
      "│    │    └─MBConv (2)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      (10,710)             False\n",
      "│    └─Sequential (3)                                        [1, 24, 56, 56]      [1, 48, 28, 28]      --                   False\n",
      "│    │    └─MBConv (0)                                       [1, 24, 56, 56]      [1, 48, 28, 28]      (16,518)             False\n",
      "│    │    └─MBConv (1)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      (43,308)             False\n",
      "│    │    └─MBConv (2)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      (43,308)             False\n",
      "│    └─Sequential (4)                                        [1, 48, 28, 28]      [1, 88, 14, 14]      --                   False\n",
      "│    │    └─MBConv (0)                                       [1, 48, 28, 28]      [1, 88, 14, 14]      (50,300)             False\n",
      "│    │    └─MBConv (1)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
      "│    │    └─MBConv (2)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
      "│    │    └─MBConv (3)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
      "│    └─Sequential (5)                                        [1, 88, 14, 14]      [1, 120, 14, 14]     --                   False\n",
      "│    │    └─MBConv (0)                                       [1, 88, 14, 14]      [1, 120, 14, 14]     (149,158)            False\n",
      "│    │    └─MBConv (1)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
      "│    │    └─MBConv (2)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
      "│    │    └─MBConv (3)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
      "│    └─Sequential (6)                                        [1, 120, 14, 14]     [1, 208, 7, 7]       --                   False\n",
      "│    │    └─MBConv (0)                                       [1, 120, 14, 14]     [1, 208, 7, 7]       (301,406)            False\n",
      "│    │    └─MBConv (1)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
      "│    │    └─MBConv (2)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
      "│    │    └─MBConv (3)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
      "│    │    └─MBConv (4)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
      "│    └─Sequential (7)                                        [1, 208, 7, 7]       [1, 352, 7, 7]       --                   False\n",
      "│    │    └─MBConv (0)                                       [1, 208, 7, 7]       [1, 352, 7, 7]       (846,900)            False\n",
      "│    │    └─MBConv (1)                                       [1, 352, 7, 7]       [1, 352, 7, 7]       (1,888,920)          False\n",
      "│    └─Conv2dNormActivation (8)                              [1, 352, 7, 7]       [1, 1408, 7, 7]      --                   False\n",
      "│    │    └─Conv2d (0)                                       [1, 352, 7, 7]       [1, 1408, 7, 7]      (495,616)            False\n",
      "│    │    └─BatchNorm2d (1)                                  [1, 1408, 7, 7]      [1, 1408, 7, 7]      (2,816)              False\n",
      "│    │    └─SiLU (2)                                         [1, 1408, 7, 7]      [1, 1408, 7, 7]      --                   --\n",
      "├─AdaptiveAvgPool2d (avgpool)                                [1, 1408, 7, 7]      [1, 1408, 1, 1]      --                   --\n",
      "├─Sequential (classifier)                                    [1, 1408]            [1, 1000]            --                   True\n",
      "│    └─Dropout (0)                                           [1, 1408]            [1, 1408]            --                   --\n",
      "│    └─Linear (1)                                            [1, 1408]            [1, 1000]            1,409,000            True\n",
      "============================================================================================================================================\n",
      "Total params: 9,114,221\n",
      "Trainable params: 1,413,227\n",
      "Non-trainable params: 7,700,994\n",
      "Total mult-adds (Units.MEGABYTES): 659.05\n",
      "============================================================================================================================================\n",
      "Input size (MB): 0.60\n",
      "Forward/backward pass size (MB): 156.81\n",
      "Params size (MB): 36.44\n",
      "Estimated Total Size (MB): 193.85\n",
      "============================================================================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[288]\n",
       "    resize_size=[288]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BICUBIC\n",
       ")"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effnetb2, effnetb2_transforms = create_effnetb2(output_feature_count=3)\n",
    "effnetb2\n",
    "print(summary(model=effnetb2,\n",
    "        input_size= (1, 3, 224, 224),\n",
    "        col_names=['input_size', 'output_size', 'num_params', 'trainable'],\n",
    "        col_width=20,\n",
    "        row_settings=['var_names']))\n",
    "\n",
    "effnetb2_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader\n",
    "train_dataloader_effnetb2, test_dataloader_effnetb2, class_names = data_loaders.create_dataloaders(train_dir=train_dir, \n",
    "                                                                                          test_dir=test_dir, # use 10% data for testing\n",
    "                                                                                          transform=effnetb2_transforms, \n",
    "                                                                                          batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 5, ['pizza', 'steak', 'sushi'])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len (train_dataloader_effnetb2), len (test_dataloader_effnetb2), class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "effnetb2_optimizer = torch.optim.Adam(effnetb2.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general trainer\n",
    "\n",
    "def train (model: torch.nn.Module,\n",
    "           train_dataloader: torch.utils.data.DataLoader,\n",
    "           test_dataloader: torch.utils.data.DataLoader,\n",
    "           optimizer: torch.optim.Optimizer,\n",
    "           loss_fn: torch.nn.Module,\n",
    "           epochs: int,\n",
    "           device: torch.device,\n",
    "           writer: torch.utils.tensorboard.writer.SummaryWriter) -> Dict[str, List]:\n",
    "    \n",
    "    results = {'train_loss':[],\n",
    "               'train_acc': [],\n",
    "               'test_loss': [],\n",
    "               'test_acc': []}\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model = model,\n",
    "                                           dataloader = train_dataloader,\n",
    "                                           loss_fn = loss_fn,\n",
    "                                           optimizer=optimizer,\n",
    "                                           device = device)\n",
    "        \n",
    "        test_loss, test_acc = test_step(model = model,\n",
    "                                        dataloader = test_dataloader,\n",
    "                                        loss_fn = loss_fn,\n",
    "                                        device=device)\n",
    "        \n",
    "        print (\n",
    "            f'Epoch: {epoch + 1} |'\n",
    "            f'train_loss: {train_loss:.4f} |'\n",
    "            f'train_acc: {train_acc:.4f} |'\n",
    "            f'test_loss {test_loss:.4f} |'\n",
    "            f'test_acc:  {test_acc}:.4f'\n",
    "\n",
    "        )\n",
    "\n",
    "        results['train_loss'].append(train_loss)\n",
    "        results['train_acc'].append(train_acc)\n",
    "        results['test_loss'].append(test_loss)\n",
    "        results['test_acc'].append(test_acc)\n",
    "\n",
    "    if writer:\n",
    "        writer.add_scalars (main_tag = 'Accuracy',\n",
    "                            tag_scalar_dict = {'train_acc': train_acc,\n",
    "                                              'test_acc':test_acc},\n",
    "                            global_step=epoch)\n",
    "        writer.add_scalars (main_tag = 'Loss',\n",
    "                           tag_scalar_dict = {'train_loss': train_loss,\n",
    "                                              'test_loss' : test_loss},\n",
    "                            global_step = epoch)\n",
    "        \n",
    "        writer.add_graph (model = model,\n",
    "                          input_to_model = torch.randn(32, 3, 224, 224).to(device))# 3 or 1?\n",
    "    \n",
    "    writer.close()\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:35<05:20, 35.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 |train_loss: 0.1022 |train_acc: 0.9875 |test_loss 0.2902 |test_acc:  0.9255681818181818:.4f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [01:08<04:32, 34.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 |train_loss: 0.0979 |train_acc: 0.9896 |test_loss 0.3408 |test_acc:  0.9130681818181818:.4f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [01:41<03:55, 33.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 |train_loss: 0.1415 |train_acc: 0.9771 |test_loss 0.2953 |test_acc:  0.9255681818181818:.4f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [02:14<03:20, 33.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 |train_loss: 0.1080 |train_acc: 0.9812 |test_loss 0.2801 |test_acc:  0.9255681818181818:.4f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [02:47<02:46, 33.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 |train_loss: 0.1210 |train_acc: 0.9812 |test_loss 0.3025 |test_acc:  0.9255681818181818:.4f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [03:20<02:11, 32.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 |train_loss: 0.0965 |train_acc: 0.9917 |test_loss 0.2833 |test_acc:  0.9443181818181818:.4f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [03:53<01:39, 33.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 |train_loss: 0.1140 |train_acc: 0.9812 |test_loss 0.2904 |test_acc:  0.9380681818181819:.4f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [04:27<01:06, 33.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 |train_loss: 0.1035 |train_acc: 0.9792 |test_loss 0.2880 |test_acc:  0.928409090909091:.4f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [05:00<00:33, 33.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 |train_loss: 0.1234 |train_acc: 0.9896 |test_loss 0.2518 |test_acc:  0.940909090909091:.4f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [05:33<00:00, 33.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 |train_loss: 0.0793 |train_acc: 0.9917 |test_loss 0.2464 |test_acc:  0.9380681818181819:.4f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train effnetb2\n",
    "set_seeds()\n",
    "\n",
    "effnetb2_retults = train (model = effnetb2,\n",
    "                          train_dataloader=train_dataloader_effnetb2,\n",
    "                          test_dataloader=test_dataloader_effnetb2,\n",
    "                          optimizer=effnetb2_optimizer,\n",
    "                          loss_fn = loss_fn,\n",
    "                          epochs = 10,\n",
    "                          device=device,\n",
    "                          writer=writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First input tensor: tensor([[[ 2.0434,  1.9920,  1.9578,  ..., -0.2342, -0.5424, -0.7993],\n",
      "         [ 2.0263,  1.9407,  1.9064,  ...,  0.1597, -0.0287, -0.2684],\n",
      "         [ 1.9749,  1.9064,  1.8722,  ...,  0.1426,  0.1254,  0.1083],\n",
      "         ...,\n",
      "         [ 2.1119,  2.0777,  2.0777,  ..., -0.2171, -0.3369, -0.3883],\n",
      "         [ 2.1119,  2.0777,  2.0777,  ..., -0.2171, -0.2342, -0.2684],\n",
      "         [ 2.0777,  2.1119,  2.1290,  ...,  0.2282, -0.2513, -0.2856]],\n",
      "\n",
      "        [[ 1.8859,  1.8333,  1.8508,  ..., -1.1604, -1.4405, -1.6681],\n",
      "         [ 1.8683,  1.7983,  1.7983,  ..., -0.7927, -0.9503, -1.1779],\n",
      "         [ 1.8508,  1.7983,  1.7633,  ..., -0.8277, -0.8277, -0.8452],\n",
      "         ...,\n",
      "         [ 1.6408,  1.6057,  1.6057,  ..., -0.6527, -0.5126, -0.4251],\n",
      "         [ 1.6583,  1.6232,  1.6057,  ..., -0.7052, -0.4426, -0.3200],\n",
      "         [ 1.6232,  1.6583,  1.6583,  ..., -0.3025, -0.4951, -0.3725]],\n",
      "\n",
      "        [[ 1.4722,  1.4722,  1.4897,  ..., -1.5430, -1.6824, -1.8044],\n",
      "         [ 1.4722,  1.4374,  1.4374,  ..., -1.1770, -1.2816, -1.4907],\n",
      "         [ 1.4897,  1.4200,  1.4025,  ..., -1.2641, -1.2467, -1.2467],\n",
      "         ...,\n",
      "         [-0.2010, -0.2010, -0.1835,  ..., -0.8284, -0.4275, -0.2010],\n",
      "         [-0.2358, -0.2358, -0.1835,  ..., -0.9504, -0.4101, -0.1312],\n",
      "         [-0.2881, -0.2358, -0.1487,  ..., -0.5670, -0.4798, -0.1835]]])\n",
      "Requires grad: False\n",
      "Grad function: None\n"
     ]
    }
   ],
   "source": [
    "# # test code to see how dtaloader work\n",
    "# # train_dataloader_effnetb2\n",
    "# # Create an iterator from the DataLoader\n",
    "# data_iter = iter(train_dataloader_effnetb2)\n",
    "\n",
    "# # Get the first batch\n",
    "# first_batch = next(data_iter)\n",
    "\n",
    "# # print(first_batch)\n",
    "\n",
    "# # \n",
    "# # Get the first batch\n",
    "# # batch = next(iter(dataloader))\n",
    "# input_batch, label_batch = first_batch\n",
    "\n",
    "# # Check the first element\n",
    "# first_input = input_batch[0]\n",
    "\n",
    "# print(f\"First input tensor: {first_input}\")\n",
    "# print(f\"Requires grad: {first_input.requires_grad}\")\n",
    "# print(f\"Grad function: {first_input.grad_fn}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py311UdemyCuda1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
