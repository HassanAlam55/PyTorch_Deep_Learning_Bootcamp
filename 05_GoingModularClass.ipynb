{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Going Modular\n",
    "Follow the class and code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiger\\anaconda3\\envs\\p311Udacity1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import plot_predictions, plot_decision_boundary, accuracy_fn\n",
    "import mlxtend\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random\n",
    "import requests\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_blobs\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchmetrics import Accuracy, ConfusionMatrix\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from timeit import default_timer as timer\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, Dict, List\n",
    "import zipfile\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dicrectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('going_modular', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('../data/pizza_steak_sushi_20_percent/train'),\n",
       " WindowsPath('../data/pizza_steak_sushi_20_percent/test'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up paths\n",
    "data_path = Path('../data/')\n",
    "image_path = data_path /'pizza_steak_sushi_20_percent'\n",
    "train_dir = image_path/'train'\n",
    "test_dir = image_path/'test'\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/data_loaders.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/data_loaders.py\n",
    "'''\n",
    "function for creating dataloader\n",
    "'''\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def create_dataloaders(\n",
    "            train_dir: str,\n",
    "            test_dir: str,\n",
    "            transform: transforms.Compose,\n",
    "            batch_size: int,\n",
    "            num_workers: int= 0,\n",
    "            pin_memory = True\n",
    "            ):\n",
    "    \n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"    \"\"\"_summary_\n",
    "    \"\"\"    \n",
    "\n",
    "    train_data = datasets.ImageFolder(train_dir, transform = transform)\n",
    "    test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "    class_names = train_data.classses\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=bath_size,\n",
    "        shuffle = True,\n",
    "        num_workers = num_workers,\n",
    "        pin_memory = pin_memory\n",
    "    )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle = False,\n",
    "        num_workers = num_workers,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader, class_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/hassan_TinyVGG.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/hassan_TinyVGG.py\n",
    "# # do the CNN to classify into one of e classes\n",
    "#  with model stacking to improve performance\n",
    "import torch\n",
    "from torch import nn\n",
    "class HassanFood(nn.Module):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"    \n",
    "    def __init__ (self, input_shape, hidden_units, output_shape,\n",
    "                  in_ConvNN_ker_size: int = 3,\n",
    "                  in_ConvNN_stirde:int = 1,\n",
    "                  in_ConvNN_pad: int = 1,\n",
    "                  in_MAXP_KerSize: int = 4,\n",
    "                  in_MAXP_stride = 1,\n",
    "                  in_batch_size = 32):\n",
    "        super().__init__()\n",
    "        self.conv1= nn.Conv2d(\n",
    "                in_channels = input_shape,\n",
    "                out_channels = hidden_units,\n",
    "                kernel_size = in_ConvNN_ker_size,\n",
    "                stride = in_ConvNN_stirde,\n",
    "                padding = in_ConvNN_pad \n",
    "                )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels = hidden_units,\n",
    "            out_channels=hidden_units,\n",
    "            kernel_size=in_ConvNN_ker_size,\n",
    "            stride=in_ConvNN_stirde,\n",
    "            padding=in_ConvNN_pad\n",
    "        )\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=in_MAXP_KerSize,\n",
    "                                    stride = in_MAXP_stride)\n",
    "        self.lazydense = nn.LazyLinear(out_features=output_shape)\n",
    "\n",
    "    def forward (self, x):\n",
    "        x = self.lazydense(self.flatten(self.maxpool(self.relu(self.conv2(self.maxpool(self.relu(self.conv1(x))))))))\n",
    "\n",
    "        return (x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing it. \n",
    "torch.manual_seed(42)\n",
    "from going_modular import hassan_TinyVGG\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "out_shape = 3\n",
    "model_custom_1 = hassan_TinyVGG.HassanFood(input_shape = 3,\n",
    "                            hidden_units = 8,\n",
    "                            output_shape = out_shape).to(device)\n",
    "model_custom_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating train and test step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/engine.py \n",
    "'''\n",
    "training and test steps\n",
    "'''\n",
    "from typing import Dict, List, Tuple\n",
    "import torch\n",
    "from torch.auto import tqdm\n",
    "\n",
    "def train_step (model: torch.nn.Module,\n",
    "                dataloader: torch.utils.data.DataLoader,\n",
    "                loss_fn: torch.nn.Module,\n",
    "                optimizer: torch.optim.Optimizer):\n",
    "\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"    \n",
    "    model.train()\n",
    "    train_loss, train_acc = 0,  0\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn (y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim = 1), dim= 1)\n",
    "        train_acc  += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "        \n",
    "    train_loss = train_loss/len(dataloader)\n",
    "    train_acc = train_acc /len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module):\n",
    "\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"    \n",
    "    model.eval()\n",
    "    \n",
    "    test_loss, test_acc = 0, 0\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            test_pred_logits = model(X)\n",
    "            loss = loss_fn (test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "    test_loss = test_loss/ len(dataloader)\n",
    "    test_acc = test_acc / len (dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "\n",
    "def train_model(model: torch.nn.Module,\n",
    "                train_dataloader: torch.utils.data.DataLoader,\n",
    "                test_dataloader:torch.utils.data.DataLoader,\n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n",
    "                epochs: int = 1):\n",
    "\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"    \n",
    "    results = {'train_loss':[],\n",
    "               'train_acc': [],\n",
    "               'test_loss':[],\n",
    "               'test_acc':[]}\n",
    "    \n",
    "    range_epochs = range(epochs)\n",
    "    for epoch in tqdm(range_epochs):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader = train_dataloader,\n",
    "                                           loss_fn = loss_fn,\n",
    "                                           optimizer=optimizer)\n",
    "        test_loss, test_acc = test_step (model=model,\n",
    "                                        dataloader=test_dataloader,\n",
    "                                        loss_fn = loss_fn)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print (\n",
    "                f'Epoch: {epoch+1} |'\n",
    "                f'train_loss: {train_loss:.4f} |'\n",
    "                f'train_acc:{train_acc:.4f} | '\n",
    "                f'test_loss: {test_loss:.4f} | '\n",
    "                f'test_acc: {test_acc:.4f}'\n",
    "            )\n",
    "            \n",
    "        results['train_loss'].append(train_loss)\n",
    "        results['train_acc'].append(train_acc)\n",
    "        results['test_loss'].append(test_loss)\n",
    "        results['test_acc'].append(test_acc)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/utils.py\n",
    "'''\n",
    "utils form odel\n",
    "'''\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "def save_model (model: torch.nn.Module,\n",
    "            target_dir: str,\n",
    "            model_name: str):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): _description_\n",
    "        target_dir (str): _description_\n",
    "        model_name (str): _description_\n",
    "    \"\"\"\n",
    "    target_dir_path = Path(target_dir)\n",
    "    target_dir_path.mkdir(parents=True,\n",
    "                        exist_ok = True)\n",
    "\n",
    "    assert model_name.endswith('.pth') or model_name.endswith('pt'), 'model_name shoule end with \".pt\", or \".pth\"'\n",
    "    model_save_path = target_dir_path/model_name\n",
    "\n",
    "    print (f'[INFO] Saving model to: {model_save_path}')\n",
    "    torch.save(obj = model.state(),\n",
    "                f=model_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py310UdemyCuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
