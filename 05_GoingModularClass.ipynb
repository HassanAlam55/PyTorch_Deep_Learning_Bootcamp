{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Going Modular\n",
    "Follow the class and code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiger\\anaconda3\\envs\\p311Udacity1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import plot_predictions, plot_decision_boundary, accuracy_fn\n",
    "import mlxtend\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random\n",
    "import requests\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.datasets import make_circles\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_blobs\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchmetrics import Accuracy, ConfusionMatrix\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from timeit import default_timer as timer\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Tuple, Dict, List\n",
    "import zipfile\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make dicrectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('going_modular', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(WindowsPath('../data/pizza_steak_sushi_20_percent/train'),\n",
       " WindowsPath('../data/pizza_steak_sushi_20_percent/test'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up paths\n",
    "data_path = Path('../data/')\n",
    "image_path = data_path /'pizza_steak_sushi_20_percent'\n",
    "train_dir = image_path/'train'\n",
    "test_dir = image_path/'test'\n",
    "train_dir, test_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/data_loaders.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/data_loaders.py\n",
    "'''\n",
    "function for creating dataloader\n",
    "'''\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def create_dataloaders(\n",
    "            train_dir: str,\n",
    "            test_dir: str,\n",
    "            transform: transforms.Compose,\n",
    "            batch_size: int,\n",
    "            num_workers: int= 0,\n",
    "            pin_memory = True\n",
    "            ):\n",
    "    \n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"    \"\"\"_summary_\n",
    "    \"\"\"    \n",
    "\n",
    "    train_data = datasets.ImageFolder(train_dir, transform = transform)\n",
    "    test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "    class_names = train_data.classes\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle = True,\n",
    "        num_workers = num_workers,\n",
    "        pin_memory = pin_memory\n",
    "    )\n",
    "\n",
    "    test_dataloader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle = False,\n",
    "        num_workers = num_workers,\n",
    "        pin_memory=pin_memory\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader, class_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/hassan_TinyVGG.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/hassan_TinyVGG.py\n",
    "# # do the CNN to classify into one of e classes\n",
    "#  with model stacking to improve performance\n",
    "import torch\n",
    "from torch import nn\n",
    "class HassanFood(nn.Module):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        nn (_type_): _description_\n",
    "    \"\"\"    \n",
    "    def __init__ (self, input_shape, hidden_units, output_shape,\n",
    "                  in_ConvNN_ker_size: int = 3,\n",
    "                  in_ConvNN_stirde:int = 1,\n",
    "                  in_ConvNN_pad: int = 1,\n",
    "                  in_MAXP_KerSize: int = 4,\n",
    "                  in_MAXP_stride = 1,\n",
    "                  in_batch_size = 32):\n",
    "        super().__init__()\n",
    "        self.conv1= nn.Conv2d(\n",
    "                in_channels = input_shape,\n",
    "                out_channels = hidden_units,\n",
    "                kernel_size = in_ConvNN_ker_size,\n",
    "                stride = in_ConvNN_stirde,\n",
    "                padding = in_ConvNN_pad \n",
    "                )\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels = hidden_units,\n",
    "            out_channels=hidden_units,\n",
    "            kernel_size=in_ConvNN_ker_size,\n",
    "            stride=in_ConvNN_stirde,\n",
    "            padding=in_ConvNN_pad\n",
    "        )\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=in_MAXP_KerSize,\n",
    "                                    stride = in_MAXP_stride)\n",
    "        self.lazydense = nn.LazyLinear(out_features=output_shape)\n",
    "\n",
    "    def forward (self, x):\n",
    "        x = self.lazydense(self.flatten(self.maxpool(self.relu(self.conv2(self.maxpool(self.relu(self.conv1(x))))))))\n",
    "\n",
    "        return (x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing it. \n",
    "torch.manual_seed(42)\n",
    "from going_modular import hassan_TinyVGG\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "out_shape = 3\n",
    "model_custom_1 = hassan_TinyVGG.HassanFood(input_shape = 3,\n",
    "                            hidden_units = 8,\n",
    "                            output_shape = out_shape).to(device)\n",
    "model_custom_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating train and test step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/engine.py \n",
    "'''\n",
    "training and test steps\n",
    "'''\n",
    "from typing import Dict, List, Tuple\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def train_step (model: torch.nn.Module,\n",
    "                dataloader: torch.utils.data.DataLoader,\n",
    "                loss_fn: torch.nn.Module,\n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                device:torch.device):\n",
    "\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"    \n",
    "    model.train()\n",
    "    train_loss, train_acc = 0,  0\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn (y_pred, y)\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim = 1), dim= 1)\n",
    "        train_acc  += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "        \n",
    "    train_loss = train_loss/len(dataloader)\n",
    "    train_acc = train_acc /len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def test_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module,\n",
    "              device: torch.device):\n",
    "\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"    \n",
    "    model.eval()\n",
    "    \n",
    "    test_loss, test_acc = 0, 0\n",
    "    with torch.inference_mode():\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            test_pred_logits = model(X)\n",
    "            loss = loss_fn (test_pred_logits, y)\n",
    "            test_loss += loss.item()\n",
    "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
    "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
    "    test_loss = test_loss/ len(dataloader)\n",
    "    test_acc = test_acc / len (dataloader)\n",
    "    return test_loss, test_acc\n",
    "\n",
    "\n",
    "def train_model(model: torch.nn.Module,\n",
    "                train_dataloader: torch.utils.data.DataLoader,\n",
    "                test_dataloader:torch.utils.data.DataLoader,\n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                device: torch.device,\n",
    "                loss_fn: torch.nn.Module = torch.nn.CrossEntropyLoss(),\n",
    "                epochs: int=1\n",
    "                ):\n",
    "\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Returns:\n",
    "        _type_: _description_\n",
    "    \"\"\"    \n",
    "    results = {'train_loss':[],\n",
    "               'train_acc': [],\n",
    "               'test_loss':[],\n",
    "               'test_acc':[]}\n",
    "    \n",
    "    range_epochs = range(epochs)\n",
    "    for epoch in tqdm(range_epochs):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader = train_dataloader,\n",
    "                                           loss_fn = loss_fn,\n",
    "                                           optimizer=optimizer,\n",
    "                                           device = device)\n",
    "        test_loss, test_acc = test_step (model=model,\n",
    "                                        dataloader=test_dataloader,\n",
    "                                        loss_fn = loss_fn,\n",
    "                                        device = device)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print (\n",
    "                f'Epoch: {epoch+1} |'\n",
    "                f'train_loss: {train_loss:.4f} |'\n",
    "                f'train_acc:{train_acc:.4f} | '\n",
    "                f'test_loss: {test_loss:.4f} | '\n",
    "                f'test_acc: {test_acc:.4f}'\n",
    "            )\n",
    "            \n",
    "        results['train_loss'].append(train_loss)\n",
    "        results['train_acc'].append(train_acc)\n",
    "        results['test_loss'].append(test_loss)\n",
    "        results['test_acc'].append(test_acc)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/utils.py\n",
    "'''\n",
    "utils form odel\n",
    "'''\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "def save_model (model: torch.nn.Module,\n",
    "            target_dir: str,\n",
    "            model_name: str):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): _description_\n",
    "        target_dir (str): _description_\n",
    "        model_name (str): _description_\n",
    "    \"\"\"\n",
    "    target_dir_path = Path(target_dir)\n",
    "    target_dir_path.mkdir(parents=True,\n",
    "                        exist_ok = True)\n",
    "\n",
    "    assert model_name.endswith('.pth') or model_name.endswith('pt'), 'model_name shoule end with \".pt\", or \".pth\"'\n",
    "    model_save_path = target_dir_path/model_name\n",
    "\n",
    "    print (f'[INFO] Saving model to: {model_save_path}')\n",
    "    torch.save(obj = model.state_dict(),\n",
    "                f=model_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test make sure model runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sys\n",
    "# import torch\n",
    "# from torchvision import transforms\n",
    "# import importlib\n",
    "\n",
    "# from going_modular import data_loaders, engine, hassan_TinyVGG, utils\n",
    "# sys.path.append('./going_modular') \n",
    "# importlib.reload(data_loaders)\n",
    "# importlib.reload(engine)\n",
    "# importlib.reload(hassan_TinyVGG)\n",
    "# importlib.reload(utils)\n",
    "# # from going_modular importlib.reload(ibkr_helper)\n",
    "# # importlib.reload(general_helper)\n",
    "\n",
    "# torch.manual_seed(42)\n",
    "# torch.cuda.manual_seed(42)\n",
    "\n",
    "\n",
    "# NUM_EPOCS = 5\n",
    "# BATCH_SIZE = 32\n",
    "# HIDDEN_UNITS = 10\n",
    "# LEARING_RATE = 0.001\n",
    "\n",
    "# train_dir = '../data/pizza_steak_sushi_20_percent/train'\n",
    "# test_dir = '../data/pizza_steak_sushi_20_percent/test'\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# data_transform = transforms.Compose([\n",
    "#     transforms.Resize(size = (224, 224)),\n",
    "#                       transforms.ToTensor()])\n",
    "\n",
    "# train_dataloader, test_datalaoder, class_names = data_loaders.create_dataloaders(\n",
    "#     train_dir=train_dir,\n",
    "#     test_dir = test_dir,\n",
    "#     transform=data_transform,\n",
    "#     batch_size=BATCH_SIZE\n",
    "\n",
    "# )\n",
    "\n",
    "# hassan_custom_model  = hassan_TinyVGG.HassanFood(\n",
    "#                         input_shape = 3,\n",
    "#                         hidden_units = 9,\n",
    "#                         output_shape= len(class_names)).to(device)\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(params=hassan_custom_model.parameters(), lr = 0.001)\n",
    "\n",
    "# from timeit import default_timer as timer \n",
    "# start_time = timer ()\n",
    "\n",
    "# model_0_results = engine.train_model(model=hassan_custom_model,\n",
    "#                                      train_dataloader=train_dataloader,\n",
    "#                                      test_dataloader=test_datalaoder,\n",
    "#                                      loss_fn = loss_fn,\n",
    "#                                      optimizer=optimizer,\n",
    "#                                      epochs=NUM_EPOCS,\n",
    "#                                      device=device)\n",
    "\n",
    "# end_time = timer()\n",
    "# print(f'[INFO] total train time : {end_time - start_time:.3f} seconds')\n",
    "\n",
    "# utils.save_model (model = hassan_custom_model,\n",
    "#            target_dir = 'models',\n",
    "#             model_name = '05_hassan_model_0.pth' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/train.py\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import importlib\n",
    "\n",
    "# from going_modular import data_loaders, engine, hassan_TinyVGG, utils\n",
    "import data_loaders, engine, hassan_TinyVGG, utils\n",
    "# sys.path.append('./going_modular') \n",
    "# importlib.reload(data_loaders)\n",
    "# importlib.reload(engine)\n",
    "# importlib.reload(hassan_TinyVGG)\n",
    "# importlib.reload(utils)\n",
    "# from going_modular importlib.reload(ibkr_helper)\n",
    "# importlib.reload(general_helper)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "\n",
    "NUM_EPOCS = 5\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_UNITS = 10\n",
    "LEARING_RATE = 0.001\n",
    "\n",
    "train_dir = '../data/pizza_steak_sushi_20_percent/train'\n",
    "test_dir = '../data/pizza_steak_sushi_20_percent/test'\n",
    "\n",
    "# train_dir = '../../data/pizza_steak_sushi_20_percent/train'\n",
    "# test_dir = '../../data/pizza_steak_sushi_20_percent/test'\n",
    "\n",
    "print(os.path.abspath(train_dir))\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(size = (224, 224)),\n",
    "                      transforms.ToTensor()])\n",
    "\n",
    "train_dataloader, test_datalaoder, class_names = data_loaders.create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    test_dir = test_dir,\n",
    "    transform=data_transform,\n",
    "    batch_size=BATCH_SIZE\n",
    "\n",
    ")\n",
    "\n",
    "hassan_custom_model  = hassan_TinyVGG.HassanFood(\n",
    "                        input_shape = 3,\n",
    "                        hidden_units = 9,\n",
    "                        output_shape= len(class_names)).to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=hassan_custom_model.parameters(), lr = 0.001)\n",
    "\n",
    "from timeit import default_timer as timer \n",
    "start_time = timer ()\n",
    "\n",
    "model_0_results = engine.train_model(model=hassan_custom_model,\n",
    "                                     train_dataloader=train_dataloader,\n",
    "                                     test_dataloader=test_datalaoder,\n",
    "                                     loss_fn = loss_fn,\n",
    "                                     optimizer=optimizer,\n",
    "                                     epochs=NUM_EPOCS,\n",
    "                                     device=device)\n",
    "\n",
    "end_time = timer()\n",
    "print(f'[INFO] total train time : {end_time - start_time:.3f} seconds')\n",
    "\n",
    "utils.save_model (model = hassan_custom_model,\n",
    "           target_dir = 'models',\n",
    "            model_name = '05_hassan_model_0.pth' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\Dropbox\\GithubRepo\\Udemy\\pytorch-deep-learning-main\\pytorch-deep-learning-main - Copy\\data\\pizza_steak_sushi_20_percent\\train\n",
      "Epoch: 1 |train_loss: 5.2459 |train_acc:0.3187 | test_loss: 1.3941 | test_acc: 0.4716\n",
      "[INFO] total train time : 15.893 seconds\n",
      "[INFO] Saving model to: models\\05_hassan_model_0.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiger\\anaconda3\\envs\\p311Udacity1\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      " 20%|██        | 1/5 [00:03<00:13,  3.38s/it]\n",
      " 40%|████      | 2/5 [00:06<00:09,  3.03s/it]\n",
      " 60%|██████    | 3/5 [00:09<00:06,  3.05s/it]\n",
      " 80%|████████  | 4/5 [00:12<00:03,  3.17s/it]\n",
      "100%|██████████| 5/5 [00:15<00:00,  3.22s/it]\n",
      "100%|██████████| 5/5 [00:15<00:00,  3.18s/it]\n"
     ]
    }
   ],
   "source": [
    "!python going_modular/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/train_argparse.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/train_argparse.py\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import importlib\n",
    "\n",
    "# from going_modular import data_loaders, engine, hassan_TinyVGG, utils\n",
    "import data_loaders, engine, hassan_TinyVGG, utils\n",
    "# sys.path.append('./going_modular') \n",
    "# importlib.reload(data_loaders)\n",
    "# importlib.reload(engine)\n",
    "# importlib.reload(hassan_TinyVGG)\n",
    "# importlib.reload(utils)\n",
    "# from going_modular importlib.reload(ibkr_helper)\n",
    "# importlib.reload(general_helper)\n",
    "\n",
    "parser = argparse.ArgumentParser(description = 'Get Hyper peramters')\n",
    "\n",
    "parser.add_argument('--num_epochs',\n",
    "                    default = 5,\n",
    "                    type= int,\n",
    "                    help = 'number of training epochs, default = 5')\n",
    "\n",
    "parser.add_argument ('--batch_size',\n",
    "                    default = 16,\n",
    "                    type=int,\n",
    "                    help='number of hidden units in layers, default = 16')\n",
    "\n",
    "parser.add_argument ('--hidden_units',\n",
    "                    default = 8,\n",
    "                    type = int,\n",
    "                    help = 'number of hidden units, default = 16')\n",
    "\n",
    "parser.add_argument ('--learning_rate',\n",
    "                    default = 0.001,\n",
    "                    type = float,\n",
    "                    help = 'learning rate, default = 0.001')\n",
    "\n",
    "parser.add_argument ('--train_dir',\n",
    "                    default = '../data/pizza_steak_sushi_20_percent/train',\n",
    "                    type = str,\n",
    "                    help = 'training directory default = ../data/pizza_steak_sushi_20_percent/train' )\n",
    "\n",
    "parser.add_argument ('--test_dir',\n",
    "                    default = '../data/pizza_steak_sushi_20_percent/test',\n",
    "                    type = str,\n",
    "                    help = 'test directory default = ../data/pizza_steak_sushi_20_percent/test ')\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "\n",
    "NUM_EPOCS = args.num_epochs\n",
    "BATCH_SIZE = args.batch_size\n",
    "HIDDEN_UNITS = args.hidden_units\n",
    "LEARING_RATE = args.learning_rate\n",
    "\n",
    "train_dir = args.train_dir\n",
    "test_dir = args.test_dir\n",
    "\n",
    "# train_dir = '../../data/pizza_steak_sushi_20_percent/train'\n",
    "# test_dir = '../../data/pizza_steak_sushi_20_percent/test'\n",
    "\n",
    "print(os.path.abspath(train_dir))\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(size = (224, 224)),\n",
    "                      transforms.ToTensor()])\n",
    "\n",
    "train_dataloader, test_datalaoder, class_names = data_loaders.create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    test_dir = test_dir,\n",
    "    transform=data_transform,\n",
    "    batch_size=BATCH_SIZE\n",
    "\n",
    ")\n",
    "\n",
    "hassan_custom_model  = hassan_TinyVGG.HassanFood(\n",
    "                        input_shape = 3,\n",
    "                        hidden_units = 9,\n",
    "                        output_shape= len(class_names)).to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=hassan_custom_model.parameters(), lr = 0.001)\n",
    "\n",
    "from timeit import default_timer as timer \n",
    "start_time = timer ()\n",
    "\n",
    "model_0_results = engine.train_model(model=hassan_custom_model,\n",
    "                                     train_dataloader=train_dataloader,\n",
    "                                     test_dataloader=test_datalaoder,\n",
    "                                     loss_fn = loss_fn,\n",
    "                                     optimizer=optimizer,\n",
    "                                     epochs=NUM_EPOCS,\n",
    "                                     device=device)\n",
    "\n",
    "end_time = timer()\n",
    "print(f'[INFO] total train time : {end_time - start_time:.3f} seconds')\n",
    "\n",
    "utils.save_model (model = hassan_custom_model,\n",
    "           target_dir = 'models',\n",
    "            model_name = '05_hassan_model_0.pth' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\Dropbox\\GithubRepo\\Udemy\\pytorch-deep-learning-main\\pytorch-deep-learning-main - Copy\\data\\pizza_steak_sushi_20_percent\\train\n",
      "Epoch: 1 |train_loss: 3.8187 |train_acc:0.4483 | test_loss: 1.1703 | test_acc: 0.4938\n",
      "[INFO] total train time : 18.205 seconds\n",
      "[INFO] Saving model to: models\\05_hassan_model_0.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiger\\anaconda3\\envs\\p311Udacity1\\Lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\n",
      " 20%|██        | 1/5 [00:03<00:14,  3.59s/it]\n",
      " 40%|████      | 2/5 [00:06<00:10,  3.35s/it]\n",
      " 60%|██████    | 3/5 [00:10<00:06,  3.45s/it]\n",
      " 80%|████████  | 4/5 [00:14<00:03,  3.72s/it]\n",
      "100%|██████████| 5/5 [00:18<00:00,  3.72s/it]\n",
      "100%|██████████| 5/5 [00:18<00:00,  3.64s/it]\n"
     ]
    }
   ],
   "source": [
    "!python going_modular/train_argparse.py --batch_size 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py310UdemyCuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
